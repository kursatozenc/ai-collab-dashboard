{
  "clusters": [
    {
      "id": "cluster-0",
      "label": "Learning & Agents & Education",
      "topTerms": [
        "learning",
        "agents",
        "education",
        "ai",
        "build"
      ],
      "sampleDesignQuestions": [
        "What does each level of the AI integration spectrum look like in practice for your team's daily workflows?",
        "Which of your team's strengths could AI amplify, and which capabilities should remain exclusively human?"
      ],
      "designFocus": "role"
    },
    {
      "id": "cluster-1",
      "label": "Robot & Teaming & Control",
      "topTerms": [
        "robot",
        "teaming",
        "control",
        "design",
        "human"
      ],
      "sampleDesignQuestions": [
        "Does your team's AI use qualify as 'teaming' or just 'interaction'? What would need to change to reach true interdependence?",
        "How should task division patterns change now that LLMs can handle increasingly complex knowledge work?"
      ],
      "designFocus": "capability boundary"
    },
    {
      "id": "cluster-2",
      "label": "Policy & Privacy & Scaling",
      "topTerms": [
        "policy",
        "privacy",
        "scaling"
      ],
      "sampleDesignQuestions": []
    },
    {
      "id": "cluster-3",
      "label": "Team & Communication & Ai",
      "topTerms": [
        "team",
        "communication",
        "ai",
        "teammates",
        "teams"
      ],
      "sampleDesignQuestions": [
        "How might we design transparency mechanisms that help teams calibrate trust appropriately rather than defaulting to over- or under-trust?",
        "What rituals could help teams repair trust after an AI teammate makes a significant error?"
      ],
      "designFocus": "interface"
    },
    {
      "id": "cluster-4",
      "label": "Assisted & Algorithmic & Management",
      "topTerms": [
        "assisted",
        "algorithmic",
        "management",
        "supporting",
        "dynamic"
      ],
      "sampleDesignQuestions": [
        "How do you ensure algorithmic management amplifies worker agency rather than constraining it?"
      ]
    }
  ],
  "nodes": [
    {
      "id": "trust-digital-teams",
      "title": "Trust in Digital Human-AI Team Collaboration: A Systematic Review",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Systematic review synthesizing trust dynamics in digital human-AI teams, identifying key factors like transparency, predictability, and shared mental models that influence trust calibration.",
      "url": "/papers/Trust%20in%20Digital%20Human-AI%20Team%20Collaboration_%20A%20Systematic%20Review_Publishers_Version.pdf",
      "source": "research",
      "designLevers": [
        "interface",
        "governance"
      ],
      "designerIntents": [
        "team_structure",
        "governance_policy"
      ],
      "designQuestion": "How might we design transparency mechanisms that help teams calibrate trust appropriately rather than defaulting to over- or under-trust?",
      "tags": [
        "trust",
        "systematic-review",
        "transparency"
      ],
      "embedding": [
        501.64484420711636,
        607.2365270097379
      ]
    },
    {
      "id": "shaping-trust",
      "title": "Shaping a Multidisciplinary Understanding of Team Trust in Human-AI Teams",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Proposes a theoretical framework bridging organizational psychology and AI research to understand how trust develops, erodes, and is repaired in human-AI team contexts.",
      "url": "/papers/Shaping%20a%20multidisciplinary%20understanding%20of%20team%20trust%20in%20human-AI%20teams%20%20a%20theoretical%20framework%20(1).pdf",
      "source": "research",
      "designLevers": [
        "ritual",
        "role"
      ],
      "designerIntents": [
        "team_structure",
        "ritual_design"
      ],
      "designQuestion": "What rituals could help teams repair trust after an AI teammate makes a significant error?",
      "tags": [
        "trust",
        "framework",
        "emerging"
      ],
      "embedding": [
        501.89125710581817,
        607.642693858251
      ]
    },
    {
      "id": "impacts-trust-model",
      "title": "IMPACTS: A Trust Model for Human-Autonomy Teaming",
      "authors": "Hou et al.",
      "year": 2021,
      "cluster": "cluster-3",
      "summary": "Introduces the IMPACTS trust model for human-autonomy teaming, decomposing trust into measurable factors including intent, measurability, performance, adaptability, communication, transparency, and security.",
      "url": "/papers/Hou_2021_IMPACTS_A_Trust_Model_Human-Autonomy_Teaming.pdf",
      "source": "research",
      "designLevers": [
        "interface",
        "governance"
      ],
      "designerIntents": [
        "governance_policy",
        "tooling_selection"
      ],
      "designQuestion": "Which of the seven IMPACTS trust factors is most critical for your team's context, and how would you measure it?",
      "tags": [
        "trust",
        "model",
        "measurement"
      ],
      "embedding": [
        501.22087400048935,
        606.9277014319691
      ]
    },
    {
      "id": "trust-ai-team-member",
      "title": "Would You Trust an AI Team Member? Team Trust in Human-AI Teams",
      "authors": "Georganta et al.",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Empirical study on trust formation with AI team members, finding that trust in AI teammates follows different trajectories than interpersonal trust and requires distinct calibration.",
      "url": "/papers/J%20Occupat%20%20%20Organ%20Psyc%20-%202024%20-%20Georganta%20-%20Would%20you%20trust%20an%20AI%20team%20member%20%20Team%20trust%20in%20human%20AI%20teams.pdf",
      "source": "research",
      "designLevers": [
        "ritual",
        "interface"
      ],
      "designerIntents": [
        "team_structure",
        "ritual_design"
      ],
      "designQuestion": "How should onboarding rituals differ when a new AI teammate joins versus a new human teammate?",
      "tags": [
        "trust",
        "empirical",
        "calibration",
        "emerging"
      ],
      "embedding": [
        503.7162759245181,
        604.5868416677714
      ]
    },
    {
      "id": "ideal-human",
      "title": "An Ideal Human: Expectations of AI Teammates in Human-AI Teaming",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Investigates what humans expect from AI teammates, finding that people project human-like qualities onto AI partners and that unmet expectations significantly impact team performance.",
      "url": "/papers/22AnIdealHuman22ExpectationsofAITeammatesinHuman-AITeaming.pdf",
      "source": "research",
      "designLevers": [
        "role",
        "interface"
      ],
      "designerIntents": [
        "role_definition",
        "team_structure"
      ],
      "designQuestion": "How might we set realistic expectations for AI teammates to avoid the disappointment cycle of anthropomorphic projection?",
      "tags": [
        "expectations",
        "anthropomorphism",
        "performance"
      ],
      "embedding": [
        504.0310641261813,
        606.7614127604113
      ]
    },
    {
      "id": "tools-to-teammates",
      "title": "From Tools to Teammates: Reconceptualizing AI in Organizations",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Explores the paradigm shift from viewing AI as a tool to treating it as a teammate, examining implications for organizational design, team composition, and management practices.",
      "url": "",
      "source": "research",
      "designLevers": [
        "role",
        "governance"
      ],
      "designerIntents": [
        "team_structure",
        "role_definition"
      ],
      "designQuestion": "Where on the tool-to-teammate spectrum should AI sit in your organization, and what changes as you move along that spectrum?",
      "tags": [
        "paradigm-shift",
        "organizational-design",
        "emerging"
      ],
      "embedding": [
        499.5484309578091,
        611.4918075367922
      ]
    },
    {
      "id": "beyond-tool-teammate",
      "title": "Beyond the Tool vs. Teammate Debate: Exploring the Spectrum",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-0",
      "summary": "Argues against the binary tool/teammate framing, proposing a spectrum of AI integration levels in teams with distinct interaction patterns and design requirements at each level.",
      "url": "",
      "source": "research",
      "designLevers": [
        "role",
        "workflow"
      ],
      "designerIntents": [
        "role_definition",
        "workflow_redesign"
      ],
      "designQuestion": "What does each level of the AI integration spectrum look like in practice for your team's daily workflows?",
      "tags": [
        "spectrum",
        "integration-levels",
        "framework"
      ],
      "embedding": [
        503.2790288491806,
        215.27910028668379
      ]
    },
    {
      "id": "who-what-teammate",
      "title": "Who/What Is My Teammate? Team Composition Considerations in Human-AI Teaming",
      "authors": "Various",
      "year": 2023,
      "cluster": "cluster-3",
      "summary": "Examines how introducing AI agents changes team composition dynamics, including role allocation, expertise distribution, and the emergence of new coordination challenges.",
      "url": "/papers/Who-What%2BIs%2BMy%2BTeammate-Team%2BComposition%2BConsiderations%2Bin%2BHuman-AI%2BTeaming.pdf",
      "source": "research",
      "designLevers": [
        "role",
        "workflow"
      ],
      "designerIntents": [
        "team_structure",
        "role_definition"
      ],
      "designQuestion": "How does adding an AI agent change the optimal size and composition of your team?",
      "tags": [
        "composition",
        "coordination",
        "role-allocation"
      ],
      "embedding": [
        501.8215802466832,
        607.6544100440857
      ]
    },
    {
      "id": "when-ai-joins",
      "title": "When AI Joins the Team: A Literature Review on Intragroup Processes",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Reviews literature on how AI integration affects intragroup processes including communication patterns, conflict resolution, decision-making, and group identity formation.",
      "url": "https://www.semanticscholar.org/paper/b1f0abc44ed30f180232b93f77fde15e7775d1b2",
      "source": "research",
      "designLevers": [
        "ritual",
        "workflow"
      ],
      "designerIntents": [
        "team_structure",
        "workflow_redesign"
      ],
      "designQuestion": "Which intragroup processes need to be redesigned when AI becomes a permanent team member?",
      "tags": [
        "intragroup",
        "processes",
        "literature-review"
      ],
      "embedding": [
        498.6934934223069,
        612.1994569874397
      ],
      "citation": "Désirée Zercher et al. (2023). When AI joins the Team: A Literature Review on Intragroup Processes and their Effect on Team Performance in Team-AI Collaboration. European Conference on Information Systems."
    },
    {
      "id": "defining-hat",
      "title": "Defining Human-AI Teaming",
      "authors": "Berretta et al.",
      "year": 2023,
      "cluster": "cluster-1",
      "summary": "Provides a comprehensive definition of human-AI teaming that distinguishes it from human-AI interaction, emphasizing interdependence, shared goals, and adaptive coordination.",
      "url": "/papers/Defining-HumanAI-Teaming.pdf",
      "source": "research",
      "designLevers": [
        "role",
        "governance"
      ],
      "designerIntents": [
        "team_structure",
        "governance_policy"
      ],
      "designQuestion": "Does your team's AI use qualify as 'teaming' or just 'interaction'? What would need to change to reach true interdependence?",
      "tags": [
        "definition",
        "interdependence",
        "coordination"
      ],
      "embedding": [
        506.04244406906406,
        82.46763752060123
      ]
    },
    {
      "id": "survey-hat-lpm",
      "title": "A Survey on Human-AI Teaming with Large Pre-Trained Models",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-1",
      "summary": "Surveys how large language models are transforming human-AI teaming, from code generation to creative collaboration, identifying new patterns of human-AI task division.",
      "url": "/papers/A%20Survey%20on%20Human-AI%20Teaming%20with%20Large%20Pre-Trained%20Models.pdf",
      "source": "research",
      "designLevers": [
        "workflow",
        "capability_boundary"
      ],
      "designerIntents": [
        "workflow_redesign",
        "tooling_selection"
      ],
      "designQuestion": "How should task division patterns change now that LLMs can handle increasingly complex knowledge work?",
      "tags": [
        "LLMs",
        "survey",
        "task-division",
        "emerging"
      ],
      "embedding": [
        505.69543649411014,
        81.77311527207053
      ]
    },
    {
      "id": "hat-empirical",
      "title": "Human-Autonomy Teaming: A Review and Analysis of the Empirical Literature",
      "authors": "O'Neill et al.",
      "year": 2020,
      "cluster": "cluster-1",
      "summary": "Comprehensive meta-analysis of empirical studies on human-autonomy teaming, identifying key moderators of team effectiveness including task complexity and automation level.",
      "url": "/papers/o-neill-et-al-2020-human-autonomy-teaming-a-review-and-analysis-of-the-empirical-literature.pdf",
      "source": "research",
      "designLevers": [
        "workflow",
        "capability_boundary"
      ],
      "designerIntents": [
        "team_structure",
        "workflow_redesign"
      ],
      "designQuestion": "At what level of task complexity does human-AI teaming outperform either humans or AI working alone?",
      "tags": [
        "meta-analysis",
        "empirical",
        "effectiveness"
      ],
      "embedding": [
        502.8662242722124,
        83.04030185113112
      ]
    },
    {
      "id": "superteams",
      "title": "Artificial Intelligence Superteams & Augmentation Strategies",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-0",
      "summary": "Examines how organizations can build AI superteams through strategic augmentation, identifying patterns where AI amplifies human strengths rather than replacing human capabilities.",
      "url": "/papers/Artificial%20Intelligence%20Superteams%20%26%20Augmentation%20Strategies_%20Inc.pdf",
      "source": "industry",
      "designLevers": [
        "role",
        "workflow"
      ],
      "designerIntents": [
        "team_structure",
        "workflow_redesign"
      ],
      "designQuestion": "Which of your team's strengths could AI amplify, and which capabilities should remain exclusively human?",
      "tags": [
        "augmentation",
        "superteams",
        "strategy"
      ],
      "embedding": [
        505.0804160322696,
        222.3856322030381
      ]
    },
    {
      "id": "team-challenges-ai",
      "title": "Team Challenges: Is Artificial Intelligence the Solution?",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Evaluates whether AI can address common team challenges including coordination failures, social loafing, and groupthink, finding mixed results depending on implementation.",
      "url": "/papers/Team%20challenges_%20Is%20artificial%20intelligence%20the%20solution_.pdf",
      "source": "research",
      "designLevers": [
        "workflow",
        "governance"
      ],
      "designerIntents": [
        "team_structure",
        "governance_policy"
      ],
      "designQuestion": "Which of your team's recurring challenges could AI help solve, and which might it inadvertently worsen?",
      "tags": [
        "challenges",
        "coordination",
        "groupthink"
      ],
      "embedding": [
        497.73377519095646,
        613.1209775527941
      ]
    },
    {
      "id": "hello-mate",
      "title": "Hello Mate! Insights from the Field on Leveraging Machine Teammates",
      "authors": "Various",
      "year": 2022,
      "cluster": "cluster-0",
      "summary": "Field study of organizations using AI teammates, revealing practical insights about onboarding AI into existing teams, managing expectations, and sustaining engagement.",
      "url": "/papers/PACIS2022_HelloMateInsightsfromtheFieldonLeveragingMachineTeammates_cameraready.pdf",
      "source": "research",
      "designLevers": [
        "ritual",
        "role"
      ],
      "designerIntents": [
        "team_structure",
        "ritual_design"
      ],
      "designQuestion": "What does a good 'first week' look like when onboarding an AI teammate into an established team?",
      "tags": [
        "field-study",
        "onboarding",
        "engagement"
      ],
      "embedding": [
        504.2267710698364,
        214.34916115081722
      ]
    },
    {
      "id": "antecedents-hat",
      "title": "Examining the Antecedents of Human-AI Team Effectiveness",
      "authors": "Siemon et al.",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Identifies key antecedents of effective human-AI teaming including task design, team composition, communication protocols, and organizational support structures.",
      "url": "",
      "source": "research",
      "designLevers": [
        "workflow",
        "governance"
      ],
      "designerIntents": [
        "team_structure",
        "governance_policy"
      ],
      "designQuestion": "Which organizational support structures are prerequisites for effective human-AI teaming?",
      "tags": [
        "antecedents",
        "effectiveness",
        "organizational-support"
      ],
      "embedding": [
        501.67079833601036,
        608.8140110659612
      ]
    },
    {
      "id": "genai-colleague",
      "title": "GenAI as a New Colleague",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-0",
      "summary": "Examines how generative AI is being integrated as a virtual colleague in knowledge work, analyzing delegation patterns and the evolution of human-AI work distribution.",
      "url": "/papers/GenAI-NewColleague.pdf",
      "source": "industry",
      "designLevers": [
        "workflow",
        "role"
      ],
      "designerIntents": [
        "workflow_redesign",
        "role_definition"
      ],
      "designQuestion": "What tasks are your knowledge workers already informally delegating to GenAI, and should those delegation patterns be formalized?",
      "tags": [
        "genai",
        "delegation",
        "knowledge-work",
        "emerging"
      ],
      "embedding": [
        503.6869123418246,
        215.4216484850915
      ]
    },
    {
      "id": "when-should-i-lead",
      "title": "When Should I Lead? Exploring Human-AI Leadership Dynamics",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Investigates dynamic leadership allocation between humans and AI in team settings, finding that optimal performance requires flexible leadership transitions based on task context.",
      "url": "https://www.semanticscholar.org/paper/8e19f8e79b61502556674204a186e121966e3213",
      "source": "research",
      "designLevers": [
        "role",
        "governance"
      ],
      "designerIntents": [
        "role_definition",
        "governance_policy"
      ],
      "designQuestion": "Under what conditions should AI take the lead in your team's workflow, and how do you signal those transitions?",
      "tags": [
        "leadership",
        "dynamics",
        "transitions"
      ],
      "embedding": [
        499.64938302524007,
        611.2769865888162
      ],
      "citation": "Inês Lobo et al. (2024). When Should I Lead or Follow: Understanding Initiative Levels in Human-AI Collaborative Gameplay. Conference on Designing Interactive Systems."
    },
    {
      "id": "requirements-ai-teammates",
      "title": "Requirements for AI Teammates",
      "authors": "Various",
      "year": 2023,
      "cluster": "cluster-3",
      "summary": "Identifies core requirements for AI systems to function as effective teammates, including situational awareness, explainability, adaptability, and appropriate autonomy levels.",
      "url": "/papers/Requirements-for-AI-teammates.pdf",
      "source": "research",
      "designLevers": [
        "capability_boundary",
        "interface"
      ],
      "designerIntents": [
        "tooling_selection",
        "role_definition"
      ],
      "designQuestion": "Which capability requirements should be non-negotiable before deploying an AI teammate in your context?",
      "tags": [
        "requirements",
        "autonomy",
        "capabilities"
      ],
      "embedding": [
        499.57006253088286,
        611.2059481720644
      ]
    },
    {
      "id": "human-loop-orgs",
      "title": "Human-in-the-Loop in Organizations",
      "authors": "Various",
      "year": 2023,
      "cluster": "cluster-3",
      "summary": "Analyzes different models of human-in-the-loop AI deployment in organizations, from human oversight to human-AI co-decision making, with implications for accountability.",
      "url": "https://www.semanticscholar.org/paper/f8956dff3ee8a15ef8a7060b5efc08a522552671",
      "source": "research",
      "designLevers": [
        "governance",
        "workflow"
      ],
      "designerIntents": [
        "governance_policy",
        "workflow_redesign"
      ],
      "designQuestion": "Where should the human-in-the-loop boundary sit in your organization's AI deployment, and who decides when to move it?",
      "tags": [
        "HITL",
        "oversight",
        "accountability"
      ],
      "embedding": [
        502.8188161534394,
        608.0383243992397
      ],
      "citation": "Y. Hua (2000). Double-loop learning control (DLC) model for reengineering: a \"yin\" and \"yang\" balanced approach for effective organizational change. Proceedings of the 2000 IEEE International Conference on Management of Innovation and Technology. ICMIT 2000. 'Management in the 21st Century' (Cat. No.00EX457)."
    },
    {
      "id": "algorithmic-management",
      "title": "Working With Machines: Algorithmic Management",
      "authors": "Various",
      "year": 2023,
      "cluster": "cluster-4",
      "summary": "Examines how algorithmic management reshapes workplace dynamics, exploring both efficiency gains and concerns about worker autonomy, surveillance, and dehumanization.",
      "url": "/papers/WorkingWithMachines-algorithmic_management.pdf",
      "source": "research",
      "designLevers": [
        "governance",
        "workflow"
      ],
      "designerIntents": [
        "governance_policy",
        "workflow_redesign"
      ],
      "designQuestion": "How do you ensure algorithmic management amplifies worker agency rather than constraining it?",
      "tags": [
        "algorithmic-management",
        "autonomy",
        "surveillance"
      ],
      "embedding": [
        119.95348951455676,
        350.7579797828294
      ]
    },
    {
      "id": "state-ai-work-anthropic",
      "title": "State of AI at Work",
      "authors": "Anthropic",
      "year": 2025,
      "cluster": "cluster-3",
      "summary": "Anthropic's industry report on current AI adoption patterns in workplaces, covering usage trends, productivity impacts, and emerging challenges in human-AI collaboration.",
      "url": "/papers/FY25_Q2_State%20of%20AI%20at%20Work-Anthropic_Final.pdf",
      "source": "industry",
      "designLevers": [
        "workflow",
        "capability_boundary"
      ],
      "designerIntents": [
        "workflow_redesign",
        "tooling_selection"
      ],
      "designQuestion": "How do your team's AI adoption patterns compare to industry benchmarks, and what gaps should you address first?",
      "tags": [
        "industry-report",
        "adoption",
        "productivity",
        "emerging"
      ],
      "embedding": [
        500.23057243108633,
        611.7908998068496
      ]
    },
    {
      "id": "structuring-ai-comm",
      "title": "Structuring AI Teammate Communication: An Exploration of AI's Communication Strategies",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Explores how AI teammates should structure their communication, finding that proactive updates, explanation depth, and communication timing significantly affect team coordination.",
      "url": "https://www.semanticscholar.org/paper/e24abbf7ed3ec09eabe326a999df0afd3de4278a",
      "source": "research",
      "designLevers": [
        "interface",
        "ritual"
      ],
      "designerIntents": [
        "workflow_redesign",
        "ritual_design"
      ],
      "designQuestion": "What communication cadence and format should your AI teammate use to keep the team informed without creating noise?",
      "tags": [
        "communication",
        "proactive-updates",
        "coordination"
      ],
      "embedding": [
        500.65258707979336,
        610.1644555625782
      ],
      "citation": "Hyosun An and Minjung Park (2026). AI tools and fashion design education: instructor perspectives on student challenges and design process tool support. Fashion and Textiles."
    },
    {
      "id": "investigating-comm",
      "title": "Investigating Human-AI Team Communication Strategies",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Studies communication strategies in human-AI teams, revealing that effective AI teammates adapt communication style based on task urgency, team member expertise, and cognitive load.",
      "url": "https://www.semanticscholar.org/paper/80b22833df9438fafd209f8338ac062eeaf971eb",
      "source": "research",
      "designLevers": [
        "interface",
        "workflow"
      ],
      "designerIntents": [
        "workflow_redesign",
        "tooling_selection"
      ],
      "designQuestion": "How should your AI teammate adjust its communication style based on the urgency of the situation?",
      "tags": [
        "communication",
        "adaptive",
        "cognitive-load"
      ],
      "embedding": [
        502.50543534871446,
        607.639644986782
      ],
      "citation": "Rui Zhang et al. (2023). Investigating AI Teammate Communication Strategies and Their Impact in Human-AI Teams for Effective Teamwork. Proc. ACM Hum. Comput. Interact.."
    },
    {
      "id": "purposeful-presentation",
      "title": "The Purposeful Presentation of AI Teammates Impacts Human Acceptance and Perception",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Demonstrates that how AI teammates are introduced and framed significantly influences human acceptance, with anthropomorphic presentations increasing trust but also raising expectations.",
      "url": "/papers/ThePurposefulPresentationofAITeammatesImpactsonHumanAcceptanceandPerception.pdf",
      "source": "research",
      "designLevers": [
        "interface",
        "role"
      ],
      "designerIntents": [
        "role_definition",
        "team_structure"
      ],
      "designQuestion": "How should you introduce and frame your AI teammate to set appropriate expectations from day one?",
      "tags": [
        "framing",
        "acceptance",
        "anthropomorphism"
      ],
      "embedding": [
        499.3025147471389,
        611.2320868263628
      ]
    },
    {
      "id": "pursuit-happiness",
      "title": "The Pursuit of Happiness: The Power and Influence of AI Teammate Emotion in Human-AI Teamwork",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Investigates how emotional expressions by AI teammates influence team dynamics, finding that AI emotional displays can boost morale but may also feel manipulative if perceived as inauthentic.",
      "url": "/papers/ThepursuitofhappinessthepowerandinfluenceofAIteammateemotioninhuman-AIteamwork.pdf",
      "source": "research",
      "designLevers": [
        "interface",
        "role"
      ],
      "designerIntents": [
        "role_definition",
        "ritual_design"
      ],
      "designQuestion": "Should your AI teammate express emotions, and how do you prevent emotional displays from feeling manipulative?",
      "tags": [
        "emotion",
        "authenticity",
        "morale",
        "emerging"
      ],
      "embedding": [
        499.18186795594806,
        611.7351839595256
      ]
    },
    {
      "id": "politeness-llms",
      "title": "Politeness in Large Language Models",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-0",
      "summary": "Analyzes politeness norms in LLM interactions, finding that social conventions like politeness significantly influence AI output quality and user satisfaction in collaborative settings.",
      "url": "https://www.semanticscholar.org/paper/752adcfa738ae00407efe43fa17aaace16a0c5aa",
      "source": "research",
      "designLevers": [
        "interface",
        "ritual"
      ],
      "designerIntents": [
        "ritual_design",
        "tooling_selection"
      ],
      "designQuestion": "What social norms should govern how your team communicates with AI, and how does that shape AI behavior?",
      "tags": [
        "politeness",
        "social-norms",
        "LLMs"
      ],
      "embedding": [
        501.81510395697956,
        217.45868206478676
      ],
      "citation": "Taoufiq Zarra and R. Chiheb (2025). The Influence of Prompt Politeness on Response Quality in Large Language Models. IEEE International Conference on Circuits and Systems for Communications."
    },
    {
      "id": "ai-explaining",
      "title": "AI Explaining Its Decisions",
      "authors": "Various",
      "year": 2023,
      "cluster": "cluster-3",
      "summary": "Reviews approaches to AI explainability in team contexts, finding that explanation format, timing, and detail level must be calibrated to the user's expertise and decision criticality.",
      "url": "/papers/AI-explaining-decisions.pdf",
      "source": "research",
      "designLevers": [
        "interface",
        "capability_boundary"
      ],
      "designerIntents": [
        "tooling_selection",
        "governance_policy"
      ],
      "designQuestion": "What level of explanation does your team actually need from AI, and when does too much explanation become a burden?",
      "tags": [
        "explainability",
        "decision-making",
        "calibration"
      ],
      "embedding": [
        497.597235867387,
        613.4874791986282
      ]
    },
    {
      "id": "amershi-guidelines",
      "title": "Guidelines for Human-AI Interaction",
      "authors": "Amershi et al. (Microsoft)",
      "year": 2019,
      "cluster": "cluster-3",
      "summary": "Foundational set of 18 design guidelines for human-AI interaction, covering initial interaction, during interaction, when wrong, and over time, widely adopted across the industry.",
      "url": "https://www.semanticscholar.org/paper/ad3cf68bae32d21f25ac142287d4a556155619d2",
      "source": "industry",
      "designLevers": [
        "interface",
        "workflow"
      ],
      "designerIntents": [
        "tooling_selection",
        "workflow_redesign"
      ],
      "designQuestion": "Which of the 18 guidelines does your AI product violate, and what would it take to bring it into alignment?",
      "tags": [
        "guidelines",
        "design-patterns",
        "foundational"
      ],
      "embedding": [
        500.50921948814783,
        611.9910045777759
      ],
      "citation": "Saleema Amershi et al. (2019). Guidelines for Human-AI Interaction. International Conference on Human Factors in Computing Systems."
    },
    {
      "id": "how-make-agents",
      "title": "How to Make Agents and Influence Teammates: Understanding the Social Dynamics",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Studies social influence dynamics in human-AI teams, finding that AI agents can shape team behavior through suggestion framing, timing, and social proof mechanisms.",
      "url": "",
      "source": "research",
      "designLevers": [
        "interface",
        "governance"
      ],
      "designerIntents": [
        "governance_policy",
        "role_definition"
      ],
      "designQuestion": "How do you prevent AI social influence from undermining human autonomy in team decision-making?",
      "tags": [
        "social-influence",
        "agents",
        "decision-making",
        "emerging"
      ],
      "embedding": [
        499.3409598436297,
        611.0376125385148
      ]
    },
    {
      "id": "mutual-tom",
      "title": "Mutual Theory of Mind in AI",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Proposes that effective human-AI teaming requires mutual theory of mind where both humans and AI maintain models of each other's knowledge, intentions, and capabilities.",
      "url": "/papers/MutualTheoryOfMind-AI.pdf",
      "source": "research",
      "designLevers": [
        "interface",
        "capability_boundary"
      ],
      "designerIntents": [
        "tooling_selection",
        "learning_upskilling"
      ],
      "designQuestion": "How well does your AI understand what you know and don't know, and how well do you understand what your AI can and can't do?",
      "tags": [
        "theory-of-mind",
        "mutual-modeling",
        "emerging"
      ],
      "embedding": [
        499.7404357779257,
        612.175480201729
      ]
    },
    {
      "id": "shared-mental-models",
      "title": "Let's Think Together: Shared Mental Models in Human-AI Teams",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Examines how shared mental models develop between humans and AI, finding that explicit model sharing and calibration exercises improve team decision-making accuracy.",
      "url": "https://www.semanticscholar.org/paper/f5c507643d84d73aea70aac546009dc72a4a72f2",
      "source": "research",
      "designLevers": [
        "ritual",
        "interface"
      ],
      "designerIntents": [
        "learning_upskilling",
        "ritual_design"
      ],
      "designQuestion": "What exercises or rituals could help your team build shared mental models with AI?",
      "tags": [
        "shared-mental-models",
        "calibration",
        "decision-making"
      ],
      "embedding": [
        499.566241240919,
        611.1945685840316
      ],
      "citation": "Beau G. Schelble et al. (2022). Let's Think Together! Assessing Shared Mental Models, Performance, and Trust in Human-Agent Teams. Proc. ACM Hum. Comput. Interact.."
    },
    {
      "id": "collective-attention",
      "title": "Collective Attention in Human-AI Teams",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Studies how attention is distributed across human-AI teams, revealing patterns of collective attention allocation that predict team performance and error rates.",
      "url": "/papers/CollectiveAttention-HumanAI-Teams.pdf",
      "source": "research",
      "designLevers": [
        "workflow",
        "interface"
      ],
      "designerIntents": [
        "workflow_redesign",
        "team_structure"
      ],
      "designQuestion": "How do you ensure critical information gets the right attention when it's distributed across human and AI team members?",
      "tags": [
        "attention",
        "performance",
        "error-rates"
      ],
      "embedding": [
        501.087282625928,
        609.2452365462728
      ]
    },
    {
      "id": "collective-intelligence",
      "title": "Human-AI Collective Intelligence",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Explores how human-AI teams can achieve collective intelligence that exceeds either human or AI capabilities alone through complementary expertise and adaptive coordination.",
      "url": "/papers/human-AI-CollectiveIntelligence.pdf",
      "source": "research",
      "designLevers": [
        "workflow",
        "role"
      ],
      "designerIntents": [
        "team_structure",
        "workflow_redesign"
      ],
      "designQuestion": "What unique forms of collective intelligence emerge when humans and AI combine their complementary strengths?",
      "tags": [
        "collective-intelligence",
        "complementarity",
        "synergy",
        "emerging"
      ],
      "embedding": [
        502.2600223242599,
        609.2696307622342
      ]
    },
    {
      "id": "leveraging-team-cognition",
      "title": "Leveraging AI for Team Cognition in Human-AI Teams",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Investigates how AI can support team cognitive processes including shared situational awareness, transactive memory, and collective sensemaking in complex environments.",
      "url": "https://www.semanticscholar.org/paper/ad13022b5d0e966ff5fa555895a4a6939d4ba70c",
      "source": "research",
      "designLevers": [
        "workflow",
        "interface"
      ],
      "designerIntents": [
        "learning_upskilling",
        "workflow_redesign"
      ],
      "designQuestion": "How could AI serve as a team's cognitive amplifier rather than a cognitive crutch?",
      "tags": [
        "team-cognition",
        "situational-awareness",
        "sensemaking"
      ],
      "embedding": [
        502.66931292538374,
        607.7900785243994
      ],
      "citation": "Jessica Williams et al. (2023). The Role of Artificial Theory of Mind in Supporting Human-Agent Teaming Interactions. Human Factors and Simulation."
    },
    {
      "id": "we-train-ai",
      "title": "We Train AI, Why Not Humans Too? Exploring Human-AI Team Training",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Argues for bidirectional training in human-AI teams where humans learn to work effectively with AI while AI systems are calibrated to team-specific interaction patterns.",
      "url": "https://www.semanticscholar.org/paper/60e3fe29c300b4e44958188158b8773c7cb7dfb4",
      "source": "research",
      "designLevers": [
        "ritual",
        "capability_boundary"
      ],
      "designerIntents": [
        "learning_upskilling",
        "ritual_design"
      ],
      "designQuestion": "What does a training program for human-AI teaming look like, and who designs the curriculum?",
      "tags": [
        "training",
        "bidirectional",
        "upskilling"
      ],
      "embedding": [
        501.8637380843807,
        609.472199733685
      ],
      "citation": "Shuai Chen and Yang Zhao (2025). Why am I willing to collaborate with AI? Exploring the desire for collaboration in human-AI hybrid group brainstorming. Kybernetes."
    },
    {
      "id": "improving-collab",
      "title": "Improving Human-AI Collaboration with AI Behaviors",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Demonstrates that specific AI behavioral patterns including proactive information sharing, uncertainty communication, and adaptive pacing significantly improve collaboration outcomes.",
      "url": "https://www.semanticscholar.org/paper/f0f6d133b4ea26fc656733042774e878bef34fd0",
      "source": "research",
      "designLevers": [
        "interface",
        "workflow"
      ],
      "designerIntents": [
        "tooling_selection",
        "workflow_redesign"
      ],
      "designQuestion": "Which AI behavioral patterns most improve collaboration in your specific work context?",
      "tags": [
        "AI-behaviors",
        "collaboration",
        "uncertainty"
      ],
      "embedding": [
        500.30658002750346,
        611.1266055109692
      ],
      "citation": "Ángel Alexander Cabrera et al. (2023). Improving Human-AI Collaboration With Descriptions of AI Behavior. Proc. ACM Hum. Comput. Interact.."
    },
    {
      "id": "skills-humans-need",
      "title": "Human-Machine Teams: What Skills Do the Humans Need?",
      "authors": "Dubrow & Orvis",
      "year": 2020,
      "cluster": "cluster-1",
      "summary": "Identifies essential human skills for effective human-machine teaming including AI literacy, adaptive coordination, calibrated trust, and collaborative problem-solving.",
      "url": "/papers/DubrowOrvis2020Human-MachineTeams-Whatskillsdothehumansneed.pdf",
      "source": "research",
      "designLevers": [
        "capability_boundary",
        "ritual"
      ],
      "designerIntents": [
        "learning_upskilling",
        "team_structure"
      ],
      "designQuestion": "Which human skills does your team need to develop to work effectively alongside AI?",
      "tags": [
        "skills",
        "AI-literacy",
        "human-development"
      ],
      "embedding": [
        502.97814890521335,
        83.8345059294324
      ]
    },
    {
      "id": "hcai-hat",
      "title": "Applying Human-Centered AI in Developing Effective Human-AI Teaming",
      "authors": "Various",
      "year": 2023,
      "cluster": "cluster-3",
      "summary": "Demonstrates how human-centered AI principles can guide the development of effective human-AI teams, balancing automation benefits with human agency and oversight.",
      "url": "/papers/applying-human-centered-ai-in-developing-effective-human-ai-teaming.pdf",
      "source": "research",
      "designLevers": [
        "governance",
        "interface"
      ],
      "designerIntents": [
        "governance_policy",
        "tooling_selection"
      ],
      "designQuestion": "How do you ensure your AI teammate's design keeps humans centered rather than sidelined?",
      "tags": [
        "human-centered-AI",
        "agency",
        "oversight"
      ],
      "embedding": [
        503.7649648544854,
        607.1953837591777
      ],
      "citation": "Wei Xu and Zaifeng Gao (2023). Applying HCAI in Developing Effective Human-AI Teaming: A Perspective from Human-AI Joint Cognitive Systems. Interactions."
    },
    {
      "id": "ethics-hat",
      "title": "Ethics in Human-AI Teaming",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Maps the ethical landscape of human-AI teaming including questions of accountability, bias amplification, power dynamics, and the moral status of AI teammates.",
      "url": "/papers/Ethics-in-HumanAI_Teaming.pdf",
      "source": "research",
      "designLevers": [
        "governance",
        "role"
      ],
      "designerIntents": [
        "governance_policy",
        "role_definition"
      ],
      "designQuestion": "Who is accountable when an AI teammate's action causes harm, and how do you establish that chain of responsibility?",
      "tags": [
        "ethics",
        "accountability",
        "bias",
        "power-dynamics"
      ],
      "embedding": [
        502.42649060698113,
        608.5491652364644
      ]
    },
    {
      "id": "towards-ethical-ai",
      "title": "Towards Ethical AI in Teams",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Proposes an ethical framework for AI integration in teams that addresses fairness in task allocation, transparency in AI reasoning, and preservation of human dignity.",
      "url": "https://www.semanticscholar.org/paper/8ceaa12aaf37fa2bdbb67f9271adc85f5555b895",
      "source": "research",
      "designLevers": [
        "governance",
        "workflow"
      ],
      "designerIntents": [
        "governance_policy",
        "workflow_redesign"
      ],
      "designQuestion": "How do you ensure AI task allocation is fair and doesn't systematically disadvantage certain team members?",
      "tags": [
        "ethics",
        "fairness",
        "dignity",
        "framework"
      ],
      "embedding": [
        500.8359687464727,
        609.8689095573975
      ],
      "citation": "Beau G. Schelble et al. (2022). Towards Ethical AI: Empirically Investigating Dimensions of AI Ethics, Trust Repair, and Performance in Human-AI Teaming. Hum. Factors."
    },
    {
      "id": "synthetic-authority",
      "title": "Synthetic Authority: The Impact of AI on Organizational Power Dynamics",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-0",
      "summary": "Examines how AI introduces new forms of authority in organizations, potentially disrupting traditional power structures and creating novel accountability challenges.",
      "url": "",
      "source": "research",
      "designLevers": [
        "governance",
        "role"
      ],
      "designerIntents": [
        "governance_policy",
        "team_structure"
      ],
      "designQuestion": "How does AI redistribute power in your organization, and who gains or loses authority?",
      "tags": [
        "authority",
        "power-dynamics",
        "organizational-change",
        "emerging"
      ],
      "embedding": [
        503.1723903596608,
        215.108384505186
      ]
    },
    {
      "id": "ai-culture",
      "title": "AI and Culture",
      "authors": "Goldberg & Srivastava",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Explores the bidirectional relationship between AI and organizational culture, examining how cultural values shape AI adoption and how AI in turn transforms workplace culture.",
      "url": "/papers/ai_and_culture_-_goldberg_and_srivastava.pdf",
      "source": "research",
      "designLevers": [
        "ritual",
        "governance"
      ],
      "designerIntents": [
        "ritual_design",
        "governance_policy"
      ],
      "designQuestion": "How is AI reshaping your team's culture, and are those cultural shifts intentional or accidental?",
      "tags": [
        "culture",
        "values",
        "organizational-change"
      ],
      "embedding": [
        498.47703893087055,
        614.0352047844651
      ]
    },
    {
      "id": "soul-of-work",
      "title": "The Soul of Work in the Age of AI",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Philosophical exploration of how AI integration challenges fundamental concepts of meaningful work, professional identity, and human purpose in the workplace.",
      "url": "/papers/Soul-of-Work-AI.pdf",
      "source": "research",
      "designLevers": [
        "role",
        "governance"
      ],
      "designerIntents": [
        "role_definition",
        "governance_policy"
      ],
      "designQuestion": "What makes work meaningful when AI can do many of the tasks that once defined your professional identity?",
      "tags": [
        "meaning",
        "identity",
        "philosophy",
        "emerging"
      ],
      "embedding": [
        498.14998318994714,
        613.7757122507711
      ]
    },
    {
      "id": "human-ai-cocreation",
      "title": "Human-AI Co-Creation",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Studies co-creative processes between humans and AI, identifying patterns where human creativity and AI capabilities combine to produce outcomes neither could achieve alone.",
      "url": "/papers/Human-AI-Cocreation.pdf",
      "source": "research",
      "designLevers": [
        "workflow",
        "interface"
      ],
      "designerIntents": [
        "workflow_redesign",
        "tooling_selection"
      ],
      "designQuestion": "What does a truly co-creative workflow between humans and AI look like, beyond just 'human prompts, AI generates'?",
      "tags": [
        "co-creation",
        "creative-process",
        "synergy"
      ],
      "embedding": [
        500.56502043204824,
        610.9628585455416
      ]
    },
    {
      "id": "human-ai-cocreativity",
      "title": "Human-AI Co-Creativity: Exploring Shared Creative Processes",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-3",
      "summary": "Examines shared creative processes in human-AI teams, finding that iterative cycles of human ideation and AI elaboration produce the most novel creative outcomes.",
      "url": "https://www.semanticscholar.org/paper/7be23d2f5af3c043322cc161714a4755a038347d",
      "source": "research",
      "designLevers": [
        "workflow",
        "ritual"
      ],
      "designerIntents": [
        "workflow_redesign",
        "ritual_design"
      ],
      "designQuestion": "How many iteration cycles between human ideation and AI elaboration produce the best creative results for your team?",
      "tags": [
        "co-creativity",
        "iteration",
        "ideation"
      ],
      "embedding": [
        501.6330632299257,
        608.819432244538
      ],
      "citation": "Syed Mohsin Abbasi et al. (2025). HYBRID INTELLIGENCE IN ART STUDIO MANAGEMENT. ShodhKosh Journal of Visual and Performing Arts."
    },
    {
      "id": "focus-modality-design",
      "title": "Focus and Modality: Defining a Roadmap to Future AI-Human Teaming in Design",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-1",
      "summary": "Maps out how AI-human teaming will evolve in design practice across different modalities, from visual to conversational to spatial, with implications for design education.",
      "url": "/papers/focus-and-modality-defining-a-roadmap-to-future-ai-human-teaming-in-design.pdf",
      "source": "research",
      "designLevers": [
        "interface",
        "capability_boundary"
      ],
      "designerIntents": [
        "tooling_selection",
        "learning_upskilling"
      ],
      "designQuestion": "Which design modality (visual, conversational, spatial) is most ripe for AI-human teaming in your practice?",
      "tags": [
        "design",
        "modality",
        "education",
        "roadmap"
      ],
      "embedding": [
        502.1406277029021,
        104.89958838267599
      ]
    },
    {
      "id": "genai-task-performance",
      "title": "Human-GenAI Collaboration to Enhance Task Performance",
      "authors": "Various",
      "year": 2024,
      "cluster": "cluster-1",
      "summary": "Empirical study showing that human-GenAI collaboration enhances task performance in knowledge work, with the largest gains in tasks requiring both analytical and creative thinking.",
      "url": "https://www.semanticscholar.org/paper/294c485ebc111bfa64ecde86d5a792be8f6f68c3",
      "source": "research",
      "designLevers": [
        "workflow",
        "capability_boundary"
      ],
      "designerIntents": [
        "workflow_redesign",
        "tooling_selection"
      ],
      "designQuestion": "Which of your team's tasks sit at the intersection of analytical and creative thinking where GenAI collaboration adds the most value?",
      "tags": [
        "task-performance",
        "knowledge-work",
        "empirical"
      ],
      "embedding": [
        499.9892605050963,
        87.06567508125448
      ],
      "citation": "Suqing Wu et al. (2025). Human-generative AI collaboration enhances task performance but undermines human’s intrinsic motivation. Scientific Reports."
    },
    {
      "id": "openai-making-ai-work-for-everyone-everywhere-our-approach-to-localization",
      "title": "Making AI work for everyone, everywhere: our approach to localization",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/our-approach-to-localization",
      "summary": "OpenAI shares its approach to AI localization, showing how globally shared frontier models can be adapted to local languages, laws, and cultures without compromising safety.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.7314691454943,
        216.6140528680433
      ]
    },
    {
      "id": "openai-korea-privacy-policy",
      "title": "Korea privacy policy",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/policies/kr-privacy-policy",
      "summary": "Korea privacy policy",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-2",
      "embedding": [
        781.771671033436,
        348.5465329604598
      ]
    },
    {
      "id": "openai-gpt-5-lowers-the-cost-of-cell-free-protein-synthesis",
      "title": "GPT-5 lowers the cost of cell-free protein synthesis",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/gpt-5-lowers-protein-synthesis-cost",
      "summary": "An autonomous lab combining OpenAI’s GPT-5 with Ginkgo Bioworks’ cloud automation cut cell-free protein synthesis costs by 40% through closed-loop experimentation.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.1774489347545,
        219.59951422904945
      ]
    },
    {
      "id": "openai-introducing-trusted-access-for-cyber",
      "title": "Introducing Trusted Access for Cyber",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/trusted-access-for-cyber",
      "summary": "OpenAI introduces Trusted Access for Cyber, a trust-based framework that expands access to frontier cyber capabilities while strengthening safeguards against misuse.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.81420345633666,
        217.5502875945057
      ]
    },
    {
      "id": "openai-introducing-openai-frontier",
      "title": "Introducing OpenAI Frontier",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/introducing-openai-frontier",
      "summary": "OpenAI Frontier is an enterprise platform for building, deploying, and managing AI agents with shared context, onboarding, permissions, and governance.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        503.2160740459287,
        217.2942871967322
      ]
    },
    {
      "id": "openai-navigating-health-questions-with-chatgpt",
      "title": "Navigating health questions with ChatGPT",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/navigating-health-questions",
      "summary": "A family shares how ChatGPT helped them prepare for critical cancer treatment decisions for their son alongside expert guidance from his doctors.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.6604383108996,
        219.51573565024123
      ]
    },
    {
      "id": "openai-gpt-5-3-codex-system-card",
      "title": "GPT-5.3-Codex System Card",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/gpt-5-3-codex-system-card",
      "summary": "GPT‑5.3-Codex is the most capable agentic coding model to date, combining the frontier coding performance of GPT‑5.2-Codex with the reasoning and professional knowledge capabilities of GPT‑5.2. ",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.72741056697663,
        218.38216971903066
      ]
    },
    {
      "id": "openai-introducing-gpt-5-3-codex",
      "title": "Introducing GPT-5.3-Codex",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/introducing-gpt-5-3-codex",
      "summary": "GPT-5.3-Codex is a Codex-native agent that pairs frontier coding performance with general reasoning to support long-horizon, real-world technical work.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.79078192116765,
        218.6703851345924
      ]
    },
    {
      "id": "openai-unlocking-the-codex-harness-how-we-built-the-app-server",
      "title": "Unlocking the Codex harness: how we built the App Server",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/unlocking-the-codex-harness",
      "summary": "Learn how to embed the Codex agent using the Codex App Server, a bidirectional JSON-RPC API powering streaming progress, tool use, approvals, and diffs.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.8259441100365,
        219.97432239354748
      ]
    },
    {
      "id": "openai-vfl-wolfsburg-turns-chatgpt-into-a-club-wide-capability",
      "title": "VfL Wolfsburg turns ChatGPT into a club-wide capability",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/vfl-wolfsburg",
      "summary": "By focusing on people, not pilots, the Bundesliga club is scaling efficiency, creativity, and knowledge—without losing its football identity.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        486.1947124627497,
        219.5300050987573
      ]
    },
    {
      "id": "openai-the-sora-feed-philosophy",
      "title": "The Sora feed philosophy",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/sora-feed-philosophy",
      "summary": "Discover the Sora feed philosophy—built to spark creativity, foster connections, and keep experiences safe with personalized recommendations, parental controls, and strong guardrails.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.5473376935563,
        219.79394290924216
      ]
    },
    {
      "id": "openai-snowflake-and-openai-partner-to-bring-frontier-intelligence-to-enterprise-data",
      "title": "Snowflake and OpenAI partner to bring frontier intelligence to enterprise data",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/snowflake-partnership",
      "summary": "OpenAI and Snowflake partner in a $200M agreement to bring frontier intelligence into enterprise data, enabling AI agents and insights directly in Snowflake.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.5904908282566,
        217.8860379430287
      ]
    },
    {
      "id": "openai-introducing-the-codex-app",
      "title": "Introducing the Codex app",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/introducing-the-codex-app",
      "summary": "Introducing the Codex app for macOS—a command center for AI coding and software development with multiple agents, parallel workflows, and long-running tasks.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.85715703697593,
        218.31589500584525
      ]
    },
    {
      "id": "openai-inside-openai-s-in-house-data-agent",
      "title": "Inside OpenAI’s in-house data agent",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/inside-our-in-house-data-agent",
      "summary": "How OpenAI built an in-house AI data agent that uses GPT-5, Codex, and memory to reason over massive datasets and deliver reliable insights in minutes.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.641503574314,
        219.41778078663623
      ]
    },
    {
      "id": "openai-taisei-corporation-shapes-the-next-generation-of-talent-with-chatgpt",
      "title": "Taisei Corporation shapes the next generation of talent with ChatGPT",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/taisei",
      "summary": "Taisei Corporation uses ChatGPT Enterprise to support HR-led talent development and scale generative AI across its global construction business.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.49524061701476,
        219.08931057685908
      ]
    },
    {
      "id": "openai-retiring-gpt-4o-gpt-4-1-gpt-4-1-mini-and-openai-o4-mini-in-chatgpt",
      "title": "Retiring GPT-4o, GPT-4.1, GPT-4.1 mini, and OpenAI o4-mini in ChatGPT",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/retiring-gpt-4o-and-older-models",
      "summary": "On February 13, 2026, alongside the previously announced retirement⁠ of GPT‑5 (Instant, Thinking, and Pro), we will retire GPT‑4o, GPT‑4.1, GPT‑4.1 mini, and OpenAI o4-mini from ChatGPT. In the API, there are no changes at this time.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.41778775937223,
        219.82133412912572
      ]
    },
    {
      "id": "openai-the-next-chapter-for-ai-in-the-eu",
      "title": "The next chapter for AI in the EU",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/the-next-chapter-for-ai-in-the-eu",
      "summary": "OpenAI launches the EU Economic Blueprint 2.0 with new data, partnerships, and initiatives to accelerate AI adoption, skills, and growth across Europe.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.6409271364211,
        216.95698550340887
      ]
    },
    {
      "id": "openai-emea-youth-wellbeing-grant",
      "title": "EMEA Youth & Wellbeing Grant",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/emea-youth-and-wellbeing-grant",
      "summary": "Apply for the EMEA Youth & Wellbeing Grant, a €500,000 program funding NGOs and researchers advancing youth safety and wellbeing in the age of AI.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.1359603323728,
        218.95255209972967
      ]
    },
    {
      "id": "openai-keeping-your-data-safe-when-an-ai-agent-clicks-a-link",
      "title": "Keeping your data safe when an AI agent clicks a link",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/ai-agent-link-safety",
      "summary": "Learn how OpenAI protects user data when AI agents open links, preventing URL-based data exfiltration and prompt injection with built-in safeguards.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.54875951551566,
        221.0598852993399
      ]
    },
    {
      "id": "openai-pvh-reimagines-the-future-of-fashion-with-openai",
      "title": "PVH reimagines the future of fashion with OpenAI",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/pvh-future-of-fashion",
      "summary": "PVH Corp., parent company of Calvin Klein and Tommy Hilfiger, is adopting ChatGPT Enterprise to bring AI into fashion design, supply chain, and consumer engagement.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.66946052299943,
        220.33130461711093
      ]
    },
    {
      "id": "openai-introducing-prism",
      "title": "Introducing Prism",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/introducing-prism",
      "summary": "Prism is a free LaTeX-native workspace with GPT-5.2 built in, helping researchers write, collaborate, and reason in one place.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.49019544433025,
        219.79682296549532
      ]
    },
    {
      "id": "openai-powering-tax-donations-with-ai-powered-personalized-recommendations",
      "title": "Powering tax donations with AI powered personalized recommendations",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/trustbank",
      "summary": "TRUSTBANK partnered with Recursive to build Choice AI using OpenAI models, delivering personalized, conversational recommendations that simplify Furusato Nozei gift discovery. A multi-agent system helps donors navigate thousands of options and find gifts that match their preferences.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.5112950153629,
        224.20967750987916
      ]
    },
    {
      "id": "openai-how-indeed-uses-ai-to-help-evolve-the-job-search",
      "title": "How Indeed uses AI to help evolve the job search",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/indeed-maggie-hulce",
      "summary": "Indeed’s CRO Maggie Hulce shares how AI is transforming job search, recruiting, and talent acquisition for employers and job seekers.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.30984181431245,
        217.76762769424462
      ]
    },
    {
      "id": "openai-unrolling-the-codex-agent-loop",
      "title": "Unrolling the Codex agent loop",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/unrolling-the-codex-agent-loop",
      "summary": "A technical deep dive into the Codex agent loop, explaining how Codex CLI orchestrates models, tools, prompts, and performance using the Responses API.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.8087126661921,
        219.11845689049574
      ]
    },
    {
      "id": "openai-scaling-postgresql-to-power-800-million-chatgpt-users",
      "title": "Scaling PostgreSQL to power 800 million ChatGPT users",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/scaling-postgresql",
      "summary": "An inside look at how OpenAI scaled PostgreSQL to millions of queries per second using replicas, caching, rate limiting, and workload isolation.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        472.77342617408647,
        219.52541014156628
      ]
    },
    {
      "id": "openai-inside-praktika-s-conversational-approach-to-language-learning",
      "title": "Inside Praktika's conversational approach to language learning",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/praktika",
      "summary": "How Praktika uses GPT-4.1 and GPT-5.2 to build adaptive AI tutors that personalize lessons, track progress, and help learners achieve real-world language fluency",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.98660755113497,
        229.45765219661385
      ]
    },
    {
      "id": "openai-inside-gpt-5-for-work-how-businesses-use-gpt-5",
      "title": "Inside GPT-5 for Work: How Businesses Use GPT-5",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/business/guides-and-resources/chatgpt-usage-and-adoption-patterns-at-work",
      "summary": "A data-driven report on how workers across industries use ChatGPT—covering adoption trends, top tasks, departmental patterns, and the future of AI at work.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.76621119555324,
        219.8342745820509
      ]
    },
    {
      "id": "openai-how-higgsfield-turns-simple-ideas-into-cinematic-social-videos",
      "title": "How Higgsfield turns simple ideas into cinematic social videos",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/higgsfield",
      "summary": "Discover how Higgsfield gives creators cinematic, social-first video output from simple inputs using OpenAI GPT-4.1, GPT-5, and Sora 2.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.91590227262543,
        219.1265933481486
      ]
    },
    {
      "id": "openai-introducing-edu-for-countries",
      "title": "Introducing Edu for Countries",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/edu-for-countries",
      "summary": "Edu for Countries is a new OpenAI initiative helping governments use AI to modernize education systems and build future-ready workforces.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        503.13591184119673,
        283.6289036399166
      ]
    },
    {
      "id": "openai-how-countries-can-end-the-capability-overhang",
      "title": "How countries can end the capability overhang",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/how-countries-can-end-the-capability-overhang",
      "summary": "Our latest report reveals stark differences in advanced AI adoption across countries and outlines new initiatives to help nations capture productivity gains from AI.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.1169710282708,
        217.3733883719887
      ]
    },
    {
      "id": "openai-horizon-1000-advancing-ai-for-primary-healthcare",
      "title": "Horizon 1000: Advancing AI for primary healthcare",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/horizon-1000",
      "summary": "OpenAI and the Gates Foundation launch Horizon 1000, a $50M pilot advancing AI capabilities for healthcare in Africa. The initiative aims to reach 1,000 clinics by 2028.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.73864285397406,
        218.65383746161018
      ]
    },
    {
      "id": "openai-stargate-community",
      "title": "Stargate Community",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/stargate-community",
      "summary": "Stargate Community plans detail a community-first approach to AI infrastructure, using locally tailored plans shaped by community input, energy needs, and workforce priorities.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.68582588092323,
        218.6091866853777
      ]
    },
    {
      "id": "openai-cisco-and-openai-redefine-enterprise-engineering-with-ai-agents",
      "title": "Cisco and OpenAI redefine enterprise engineering with AI agents",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/cisco",
      "summary": "Cisco and OpenAI redefine enterprise engineering with Codex, an AI software agent embedded in workflows to speed builds, automate defect fixes, and enable AI-native development.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        503.4919046983396,
        216.19009094600582
      ]
    },
    {
      "id": "openai-servicenow-powers-actionable-enterprise-ai-with-openai",
      "title": "ServiceNow powers actionable enterprise AI with OpenAI",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/servicenow-powers-actionable-enterprise-ai-with-openai",
      "summary": "ServiceNow expands access to OpenAI frontier models to power AI-driven enterprise workflows, summarization, search, and voice across the ServiceNow Platform.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        503.36464898706413,
        216.2158173052298
      ]
    },
    {
      "id": "openai-our-approach-to-age-prediction",
      "title": "Our approach to age prediction",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/our-approach-to-age-prediction",
      "summary": "ChatGPT is rolling out age prediction to estimate if accounts are under or over 18, applying safeguards for teens and refining accuracy over time.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.28652594557275,
        219.86052254844935
      ]
    },
    {
      "id": "openai-ai-for-self-empowerment",
      "title": "AI for self empowerment",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/ai-for-self-empowerment",
      "summary": "How AI can expand human agency by closing the capability overhang—helping people, businesses, and countries unlock real productivity, growth, and opportunity.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        504.26334931937265,
        215.315812387452
      ]
    },
    {
      "id": "openai-a-business-that-scales-with-the-value-of-intelligence",
      "title": "A business that scales with the value of intelligence",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/a-business-that-scales-with-the-value-of-intelligence",
      "summary": "OpenAI’s business model scales with intelligence—spanning subscriptions, API, ads, commerce, and compute—driven by deepening ChatGPT adoption.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.4846211424078,
        219.1231760990828
      ]
    },
    {
      "id": "openai-the-truth-left-out-from-elon-musk-s-recent-court-filing",
      "title": "The truth left out from Elon Musk’s recent court filing",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/the-truth-elon-left-out",
      "summary": "The truth left out from Elon Musk’s recent court filing.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.60769881713344,
        220.55699344880708
      ]
    },
    {
      "id": "openai-introducing-chatgpt-go-now-available-worldwide",
      "title": "Introducing ChatGPT Go, now available worldwide",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/introducing-chatgpt-go",
      "summary": "ChatGPT Go is now available worldwide, offering expanded access to GPT-5.2 Instant, higher usage limits, and longer memory—making advanced AI more affordable globally.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.0120951188189,
        218.47712671985053
      ]
    },
    {
      "id": "openai-our-approach-to-advertising-and-expanding-access-to-chatgpt",
      "title": "Our approach to advertising and expanding access to ChatGPT",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/our-approach-to-advertising-and-expanding-access",
      "summary": "OpenAI plans to test advertising in the U.S. for ChatGPT’s free and Go tiers to expand affordable access to AI worldwide, while protecting privacy, trust, and answer quality.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        478.5861233537332,
        217.2402995255598
      ]
    },
    {
      "id": "openai-investing-in-merge-labs",
      "title": "Investing in Merge Labs",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/investing-in-merge-labs",
      "summary": "OpenAI is investing in Merge Labs to support new brain computer interfaces that bridge biological and artificial intelligence to maximize human ability, agency, and experience.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.3352558594087,
        218.04267775593476
      ]
    },
    {
      "id": "openai-strengthening-the-u-s-ai-supply-chain-through-domestic-manufacturing",
      "title": "Strengthening the U.S. AI supply chain through domestic manufacturing",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/strengthening-the-us-ai-supply-chain",
      "summary": "OpenAI launches a new RFP to strengthen the U.S. AI supply chain by accelerating domestic manufacturing, creating jobs, and scaling AI infrastructure.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        490.49239162544103,
        216.89072023901159
      ]
    },
    {
      "id": "openai-openai-partners-with-cerebras",
      "title": "OpenAI partners with Cerebras  ",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/cerebras-partnership",
      "summary": "OpenAI partners with Cerebras to add 750MW of high-speed AI compute, reducing inference latency and making ChatGPT faster for real-time AI workloads.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.7304492505641,
        223.41190485305177
      ]
    },
    {
      "id": "openai-zenken-boosts-a-lean-sales-team-with-chatgpt-enterprise",
      "title": "Zenken boosts a lean sales team with ChatGPT Enterprise",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/zenken",
      "summary": "By rolling out ChatGPT Enterprise company-wide, Zenken has boosted sales performance, cut preparation time, and increased proposal success rates. AI-supported workflows are helping a lean team deliver more personalized, effective customer engagement.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.723517905807,
        216.8801019995593
      ]
    },
    {
      "id": "openai-openai-s-raising-concerns-policy",
      "title": "OpenAI’s Raising Concerns Policy",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/openai-raising-concerns-policy",
      "summary": "We’re publishing our Raising Concerns Policy, which protects employees’ rights to make protected disclosures.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-2",
      "embedding": [
        1027.0543371551373,
        351.37274071180843
      ]
    },
    {
      "id": "openai-openai-and-softbank-group-partner-with-sb-energy",
      "title": "OpenAI and SoftBank Group partner with SB Energy",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/stargate-sb-energy-partnership",
      "summary": "OpenAI and SoftBank Group partner with SB Energy to develop multi-gigawatt AI data center campuses, including a 1.2 GW Texas facility supporting the Stargate initiative.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.48453188112154,
        219.45455367887124
      ]
    },
    {
      "id": "openai-datadog-uses-codex-for-system-level-code-review",
      "title": "Datadog uses Codex for system-level code review",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/datadog",
      "summary": "OpenAI and Datadog brand graphic with the OpenAI wordmark on the left, the Datadog logo on the right, and a central abstract brown fur-like texture panel on a white background.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.4904489774784,
        219.62051400886628
      ]
    },
    {
      "id": "openai-netomi-s-lessons-for-scaling-agentic-systems-into-the-enterprise",
      "title": "Netomi’s lessons for scaling agentic systems into the enterprise",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/netomi",
      "summary": "How Netomi scales enterprise AI agents using GPT-4.1 and GPT-5.2—combining concurrency, governance, and multi-step reasoning for reliable production workflows.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        469.2985022478005,
        218.608510852335
      ]
    },
    {
      "id": "openai-openai-for-healthcare",
      "title": "OpenAI for Healthcare",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/openai-for-healthcare",
      "summary": "OpenAI for Healthcare enables secure, enterprise-grade AI that supports HIPAA compliance—reducing administrative burden and supporting clinical workflows.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.79168940296324,
        218.46006863806943
      ]
    },
    {
      "id": "openai-how-tolan-builds-voice-first-ai-with-gpt-5-1",
      "title": "How Tolan builds voice-first AI with GPT-5.1",
      "authors": "Various",
      "year": 2026,
      "url": "https://openai.com/index/tolan",
      "summary": "Tolan built a voice-first AI companion with GPT-5.1, combining low-latency responses, real-time context reconstruction, and memory-driven personalities for natural conversations.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "openai",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.9420750782418,
        216.92337308833999
      ]
    },
    {
      "id": "anthropic-responsible-scaling-policy",
      "title": "Responsible Scaling Policy",
      "authors": "Various",
      "year": 2026,
      "url": "https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy",
      "summary": "",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "anthropic",
        "industry"
      ],
      "cluster": "cluster-2",
      "embedding": [
        825.1739918114266,
        350.0807263277317
      ]
    },
    {
      "id": "google-ai-natively-adaptive-interfaces-a-new-framework-for-ai-accessibility",
      "title": "Natively Adaptive Interfaces: A new framework for AI accessibility",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/company-news/outreach-and-initiatives/accessibility/natively-adaptive-interfaces-ai-accessibility/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Natively_Adaptive_Interfaces_He.max-600x600.format-webp.webp\">Learn how Google's NAI framework uses AI to make technology more adaptive, inclusive and helpful for everyone.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-3",
      "embedding": [
        497.58893356355014,
        615.2604565719956
      ]
    },
    {
      "id": "google-ai-how-google-cloud-is-helping-team-usa-elevate-their-tricks-with-ai",
      "title": "How Google Cloud is helping Team USA elevate their tricks with AI",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/us-ski-snowboard-tool-winter-olympics-2026/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/helping_Team_USA_Hero.max-600x600.format-webp.webp\">Google Cloud built an industry-first AI tool to help U.S. Ski and Snowboard athletes.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-3",
      "embedding": [
        497.80469111738097,
        613.9206503731871
      ]
    },
    {
      "id": "google-ai-watch-our-new-gemini-ad-ahead-of-football-s-biggest-weekend",
      "title": "Watch our new Gemini ad ahead of football’s biggest weekend",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/company-news/inside-google/company-announcements/gemini-ad-new-home/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/SB_2026_New_Home_16_9_Thumbnail.max-600x600.format-webp.webp\">Learn more about Google’s new ad that will run during football’s Big Game on February 8.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.3409626078852,
        219.7165105188052
      ]
    },
    {
      "id": "google-ai-the-latest-ai-news-we-announced-in-january",
      "title": "The latest AI news we announced in January",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/innovation-and-ai/products/google-ai-updates-january-2026/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/LatestAI_v5.max-600x600.format-webp.webp\">Google AI announcements from January",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-3",
      "embedding": [
        498.5662442892942,
        614.1389847851314
      ]
    },
    {
      "id": "google-ai-how-we-re-helping-preserve-the-genetic-information-of-endangered-species-with-ai",
      "title": "How we’re helping preserve the genetic information of endangered species with AI",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/innovation-and-ai/technology/ai/ai-to-preserve-endangered-species/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Terria-Clay_Collage_hero.max-600x600.format-webp.webp\">Scientists are working to sequence the genome of every known species on Earth.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.8271133959581,
        217.65054634678543
      ]
    },
    {
      "id": "google-ai-advancing-ai-benchmarking-with-game-arena",
      "title": "Advancing AI benchmarking with Game Arena",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/kaggle-game-arena-updates/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/kaggle_Gsmes_Hero.png\">We’re expanding Game Arena with Poker and Werewolf, while Gemini 3 Pro and Flash top our chess leaderboard.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.62366425834654,
        217.90223509622004
      ]
    },
    {
      "id": "google-ai-project-genie-experimenting-with-infinite-interactive-worlds",
      "title": "Project Genie: Experimenting with infinite, interactive worlds",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/project-genie/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/genie-3__project-genie__hero-fi.max-600x600.format-webp_d7CisM6.webp\">Google AI Ultra subscribers in the U.S. can now try out Project Genie.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.5508974401804,
        218.95943534512318
      ]
    },
    {
      "id": "google-ai-hear-more-about-interactive-world-models-in-our-latest-podcast",
      "title": "Hear more about interactive world models in our latest podcast.",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/innovation-and-ai/technology/ai/release-notes-podcast-project-genie/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/RN25_Episode_Thumbnail.max-600x600.format-webp.webp\">The latest episode of the Google AI: Release Notes podcast focuses on Genie 3, a real-time, interactive world model. Host Logan Kilpatrick chats with Diego Rivas, Shlomi…",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.31438433608025,
        218.90409024984942
      ]
    },
    {
      "id": "google-ai-google-ai-plus-is-now-available-everywhere-our-ai-plans-are-available-including-the-u-s",
      "title": "Google AI Plus is now available everywhere our AI plans are available, including the U.S.",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/products-and-platforms/products/google-one/google-ai-plus-availability/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Plus_Hero_Visual_2096.max-600x600.format-webp_4ffr2GI.webp\">We’re launching Google AI Plus in 35 new countries and territories including the US, making it available everywhere Google AI plans are available.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-3",
      "embedding": [
        499.23753836300807,
        613.335500106437
      ]
    },
    {
      "id": "google-ai-just-ask-anything-a-seamless-new-search-experience",
      "title": "Just ask anything: a seamless new Search experience",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/products-and-platforms/products/search/ai-mode-ai-overviews-updates/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Seamless_Search_-_Blog_header.max-600x600.format-webp.webp\">Search users around the world now have easier access to frontier AI capabilities.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.40533031098477,
        218.06020381529018
      ]
    },
    {
      "id": "google-ai-in-our-latest-podcast-hear-how-the-smoke-jumpers-team-brings-gemini-to-billions-of-people",
      "title": "In our latest podcast, hear how the “Smoke Jumpers” team brings Gemini to billions of people.",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/products-and-platforms/products/gemini/release-notes-podcast-smokejumpers/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/thumbnails_EP24_002_ccRelease_N.max-600x600.format-webp.webp\">Bringing Gemini to billions of users requires a massive, coordinated infrastructure effort. In the latest episode of the Google AI: Release Notes podcast, host Logan Kil…",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.0738497004451,
        217.48174972992248
      ]
    },
    {
      "id": "google-ai-how-animators-and-ai-researchers-made-dear-upstairs-neighbors",
      "title": "How animators and AI researchers made ‘Dear Upstairs Neighbors’",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/dear-upstairs-neighbors/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DUN_poster_16x9_v02.max-600x600.format-webp.webp\">Today, our animated short film, “Dear Upstairs Neighbors,” previews at the Sundance Film Festival.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.25482869735606,
        218.0743422192365
      ]
    },
    {
      "id": "google-ai-personal-intelligence-in-ai-mode-in-search-help-that-s-uniquely-yours",
      "title": "Personal Intelligence in AI Mode in Search: Help that's uniquely yours",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/products-and-platforms/products/search/personal-intelligence-ai-mode-search/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/pcontext_sizzle_thumbnail.max-600x600.format-webp.webp\">Personal Intelligence lets you tap into your context from Gmail and Photos to deliver tailored responses in Search, just for you.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.42822455427023,
        217.56277205303624
      ]
    },
    {
      "id": "google-ai-building-a-community-led-future-for-ai-in-film-with-sundance-institute",
      "title": "Building a community-led future for AI in film with Sundance Institute",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/company-news/outreach-and-initiatives/google-org/sundance-institute-ai-education/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Building_a_community-led_future.max-600x600.format-webp.webp\">A look at how Sundance Institute will build a community-led ecosystem for AI education and empowerment, to support creatives.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        503.3823841169148,
        249.67186083129351
      ]
    },
    {
      "id": "google-ai-how-nano-banana-got-its-name",
      "title": "How Nano Banana got its name",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/products-and-platforms/products/gemini/how-nano-banana-got-its-name/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/NanoBananaName_Hero.max-600x600.format-webp.webp\">We’re peeling back the origin story of Nano Banana, one of Google DeepMind’s most popular models.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.123483026456,
        219.77396187368805
      ]
    },
    {
      "id": "google-ai-learners-and-educators-are-ai-s-new-super-users",
      "title": "Learners and educators are AI’s new “super users”",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/products-and-platforms/products/education/our-life-with-ai-2025/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/aiineducatiuonJan2026_hero_v2.max-600x600.format-webp.webp\">Google’s 2025 Our Life with AI survey found people are using AI tools to learn new things.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-3",
      "embedding": [
        498.5757798049426,
        614.2264076395011
      ]
    },
    {
      "id": "google-ai-introducing-community-benchmarks-on-kaggle",
      "title": "Introducing Community Benchmarks on Kaggle",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/innovation-and-ai/technology/developers-tools/kaggle-community-benchmarks/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/hero-final.max-600x600.format-webp.webp\">Community Benchmarks on Kaggle lets the community build, share and run custom evaluations for AI models.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.4965657502243,
        235.07309396538182
      ]
    },
    {
      "id": "google-ai-announcing-the-winner-of-the-global-ai-film-award",
      "title": "Announcing the winner of the Global AI Film Award",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/company-news/inside-google/around-the-globe/google-middle-east/winner-of-the-global-ai-film-award/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/FAYZ0207.max-600x600.format-webp.webp\">Over the past year, we’ve witnessed how creators globally have been using our AI models and tools to share their stories with the world. That’s why we launched the AI Fi…",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        503.3889559171488,
        216.26266954045195
      ]
    },
    {
      "id": "google-ai-veo-3-1-ingredients-to-video-more-consistency-creativity-and-control",
      "title": "Veo 3.1 Ingredients to Video: More consistency, creativity and control",
      "authors": "Various",
      "year": 2026,
      "url": "https://blog.google/innovation-and-ai/technology/ai/veo-3-1-ingredients-to-video/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/veo-3-1_keyword_blog_header_209.max-600x600.format-webp.webp\">Today, we’re introducing an enhanced version of Veo 3.1 “Ingredients to Video.”",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-1",
      "embedding": [
        495.1993143044385,
        93.18974851133197
      ]
    },
    {
      "id": "google-ai-2025-at-google",
      "title": "2025 at Google",
      "authors": "Various",
      "year": 2025,
      "url": "https://blog.google/innovation-and-ai/technology/ai/look-back-2025/",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/EOY_2025_Header.max-600x600.format-webp.webp\">Learn more about Google’s launches, milestones and more from 2025.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "google",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.4833385716571,
        219.71840623845418
      ]
    },
    {
      "id": "microsoft-research-rethinking-imitation-learning-with-predictive-inverse-dynamics-models",
      "title": "Rethinking imitation learning with Predictive Inverse Dynamics Models",
      "authors": "Various",
      "year": 2026,
      "url": "https://www.microsoft.com/en-us/research/blog/rethinking-imitation-learning-with-predictive-inverse-dynamics-models/",
      "summary": "<p>This research looks at why Predictive Inverse Dynamics Models often outperform standard Behavior Cloning in imitation learning. By using simple predictions of what happens next, PIDMs reduce ambiguity and learn from far fewer demonstrations.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/rethinking-imitation-learning-with-predictive-inverse-dynamics-models/\">Rethinking imitation learning with Predictive Inverse Dynamics Models</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "microsoft",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.98442490166514,
        218.65638269971942
      ]
    },
    {
      "id": "microsoft-research-paza-introducing-automatic-speech-recognition-benchmarks-and-models-for-low-resource-languages",
      "title": "Paza: Introducing automatic speech recognition benchmarks and models for low resource languages",
      "authors": "Various",
      "year": 2026,
      "url": "https://www.microsoft.com/en-us/research/blog/paza-introducing-automatic-speech-recognition-benchmarks-and-models-for-low-resource-languages/",
      "summary": "<p>Microsoft Research unveils Paza, a human-centered speech pipeline, and PazaBench, the first leaderboard for low-resource languages. It covers 39 African languages and 52 models and is tested with communities in real settings. </p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/paza-introducing-automatic-speech-recognition-benchmarks-and-models-for-low-resource-languages/\">Paza: Introducing automatic speech recognition benchmarks and models for low resource languages</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "microsoft",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.8610077325712,
        218.85207993545313
      ]
    },
    {
      "id": "microsoft-research-unirg-scaling-medical-imaging-report-generation-with-multimodal-reinforcement-learning",
      "title": "UniRG: Scaling medical imaging report generation with multimodal reinforcement learning",
      "authors": "Various",
      "year": 2026,
      "url": "https://www.microsoft.com/en-us/research/blog/unirg-scaling-medical-imaging-report-generation-with-multimodal-reinforcement-learning/",
      "summary": "<p>AI can help generate medical image reports, but today’s models struggle with varying reporting schemes. Learn how UniRG uses reinforcement learning to boost performance of medical vision-language models.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/unirg-scaling-medical-imaging-report-generation-with-multimodal-reinforcement-learning/\">UniRG: Scaling medical imaging report generation with multimodal reinforcement learning</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "microsoft",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        473.98313903103036,
        218.5488410948158
      ]
    },
    {
      "id": "microsoft-research-multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents",
      "title": "Multimodal reinforcement learning with agentic verifier for AI agents",
      "authors": "Various",
      "year": 2026,
      "url": "https://www.microsoft.com/en-us/research/blog/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents/",
      "summary": "<p>Argos improves multimodal RL by evaluating whether an agent’s reasoning aligns with what it observes over time. The approach reduces visual hallucinations and produces more reliable, data-efficient agents for real-world applications.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents/\">Multimodal reinforcement learning with agentic verifier for AI agents</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "microsoft",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.9503748879,
        217.44914967992653
      ]
    },
    {
      "id": "microsoft-research-optimind-a-small-language-model-with-optimization-expertise",
      "title": "OptiMind: A small language model with optimization expertise",
      "authors": "Various",
      "year": 2026,
      "url": "https://www.microsoft.com/en-us/research/blog/optimind-a-small-language-model-with-optimization-expertise/",
      "summary": "<p>OptiMind is a small language model that converts business operation challenges, described naturally, into mathematical formulations that optimization software can solve. It reduces formulation time &#038; errors &#038; enables fast, privacy-preserving local use.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/optimind-a-small-language-model-with-optimization-expertise/\">OptiMind: A small language model with optimization expertise</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "microsoft",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        486.6246598311289,
        218.8399719842591
      ]
    },
    {
      "id": "microsoft-research-agent-lightning-adding-reinforcement-learning-to-ai-agents-without-code-rewrites",
      "title": "Agent Lightning: Adding reinforcement learning to AI agents without code rewrites",
      "authors": "Various",
      "year": 2025,
      "url": "https://www.microsoft.com/en-us/research/blog/agent-lightning-adding-reinforcement-learning-to-ai-agents-without-code-rewrites/",
      "summary": "<p>By decoupling how agents work from how they’re trained, Agent Lightning turns each step an agent takes into data for reinforcement learning. This makes it easy for developers to improve agent performance with almost zero code changes.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/agent-lightning-adding-reinforcement-learning-to-ai-agents-without-code-rewrites/\">Agent Lightning: Adding reinforcement learning to AI agents without code rewrites</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "microsoft",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.1716153942154,
        217.72370704659085
      ]
    },
    {
      "id": "microsoft-research-promptions-helps-make-ai-prompting-more-precise-with-dynamic-ui-controls",
      "title": "Promptions helps make AI prompting more precise with dynamic UI controls",
      "authors": "Various",
      "year": 2025,
      "url": "https://www.microsoft.com/en-us/research/blog/promptions-helps-make-ai-prompting-more-precise-with-dynamic-ui-controls/",
      "summary": "<p>Promptions helps developers add dynamic, context-aware controls to chat interfaces so users can guide generative AI responses. It lets users shape outputs quickly without writing long instructions.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/promptions-helps-make-ai-prompting-more-precise-with-dynamic-ui-controls/\">Promptions helps make AI prompting more precise with dynamic UI controls</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "microsoft",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        501.68524327607287,
        217.68163403311206
      ]
    },
    {
      "id": "microsoft-research-gigatime-scaling-tumor-microenvironment-modeling-using-virtual-population-generated-by-multimodal-ai",
      "title": "GigaTIME: Scaling tumor microenvironment modeling using virtual population generated by multimodal AI",
      "authors": "Various",
      "year": 2025,
      "url": "https://www.microsoft.com/en-us/research/blog/gigatime-scaling-tumor-microenvironment-modeling-using-virtual-population-generated-by-multimodal-ai/",
      "summary": "<p>Using AI-generated virtual populations, Microsoft researchers uncovered hidden cellular patterns that could reshape how we understand and treat cancer. </p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/gigatime-scaling-tumor-microenvironment-modeling-using-virtual-population-generated-by-multimodal-ai/\">GigaTIME: Scaling tumor microenvironment modeling using virtual population generated by multimodal AI</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "microsoft",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        478.71988580770375,
        217.43652406708824
      ]
    },
    {
      "id": "microsoft-research-ideas-community-building-machine-learning-and-the-future-of-ai",
      "title": "Ideas: Community building, machine learning, and the future of AI",
      "authors": "Various",
      "year": 2025,
      "url": "https://www.microsoft.com/en-us/research/podcast/ideas-community-building-machine-learning-and-the-future-of-ai/",
      "summary": "<p>As the Women in Machine Learning Workshop (WiML) marks its 20th annual gathering, cofounders, friends, and collaborators Jenn Wortman Vaughan and Hanna Wallach reflect on WiML’s evolution, navigating the field of ML, and their work in responsible AI.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/podcast/ideas-community-building-machine-learning-and-the-future-of-ai/\">Ideas: Community building, machine learning, and the future of AI</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "microsoft",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.23377893137507,
        219.4264062102053
      ]
    },
    {
      "id": "microsoft-research-reducing-privacy-leaks-in-ai-two-approaches-to-contextual-integrity",
      "title": "Reducing Privacy leaks in AI: Two approaches to contextual integrity",
      "authors": "Various",
      "year": 2025,
      "url": "https://www.microsoft.com/en-us/research/blog/reducing-privacy-leaks-in-ai-two-approaches-to-contextual-integrity/",
      "summary": "<p>New research explores two ways to give AI agents stronger privacy safeguards grounded in contextual integrity. One adds lightweight, inference-time checks; the other builds contextual awareness directly into models through reasoning and RL.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/reducing-privacy-leaks-in-ai-two-approaches-to-contextual-integrity/\">Reducing Privacy leaks in AI: Two approaches to contextual integrity </a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "microsoft",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        456.69793609687537,
        216.61985261869592
      ]
    },
    {
      "id": "meta-ai-how-generational-differences-affect-consumer-attitudes-towards-ads",
      "title": "How generational differences affect consumer attitudes towards ads",
      "authors": "Various",
      "year": 2023,
      "url": "https://research.facebook.com/blog/2023/5/how-generational-differences-affect-consumer-attitudes-towards-ads/",
      "summary": "Our research study, in collaboration with CrowdDNA, aims to understand people's relationship with social media ads across different social media platforms.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "meta",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        500.31604297773526,
        218.49620263261593
      ]
    },
    {
      "id": "meta-ai-every-tree-counts",
      "title": "Every tree counts",
      "authors": "Various",
      "year": 2023,
      "url": "https://research.facebook.com/blog/2023/4/every-tree-counts-large-scale-mapping-of-canopy-height-at-the-resolution-of-individual-trees/",
      "summary": "Meta set a goal to reach net zero emissions by 2030. We are developing technology to mitigate our carbon footprint and making these openly available.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "meta",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.71225120030675,
        219.46445908587754
      ]
    },
    {
      "id": "meta-ai-how-a-non-traditional-background-led-to-cutting-edge-xr-tech",
      "title": "How a non-traditional background led to cutting-edge XR tech",
      "authors": "Various",
      "year": 2023,
      "url": "https://www.metacareers.com/life/how-a-non-traditional-background-led-to-cutting-edge-xr-tech",
      "summary": "",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "meta",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.2890567993471,
        219.64012931936097
      ]
    },
    {
      "id": "meta-ai-a-new-unique-ai-dataset-for-animating-amateur-drawings",
      "title": "A new, unique AI dataset for animating amateur drawings",
      "authors": "Various",
      "year": 2023,
      "url": "https://ai.facebook.com/blog/ai-dataset-animation-drawings/",
      "summary": "",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "meta",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.81966734333577,
        216.75872139094923
      ]
    },
    {
      "id": "meta-ai-how-the-metaverse-can-transform-education",
      "title": "How the metaverse can transform education",
      "authors": "Various",
      "year": 2023,
      "url": "https://about.fb.com/news/2023/04/how-the-metaverse-can-transform-education/",
      "summary": "",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "meta",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        504.3383494495366,
        483.68778887548274
      ]
    },
    {
      "id": "meta-ai-build-faster-with-buck2-our-open-source-build-system",
      "title": "Build faster with Buck2: Our open source build system",
      "authors": "Various",
      "year": 2023,
      "url": "https://engineering.fb.com/2023/04/06/open-source/buck2-open-source-large-scale-build-system/",
      "summary": "",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "meta",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        504.04694072242023,
        340.64242125239355
      ]
    },
    {
      "id": "meta-ai-announcing-the-2023-meta-research-phd-fellowship-award-winners",
      "title": "Announcing the 2023 Meta Research PhD Fellowship award winners",
      "authors": "Various",
      "year": 2023,
      "url": "https://research.facebook.com/blog/2023/4/announcing-the-2023-meta-research-phd-fellowship-award-winners/",
      "summary": "<palette xmlns:mcr=\"palette-mc-research-toolkit\" xmlns:mcr-blog=\"palette-mc-research-blog-toolkit\" xmlns:meta-props=\"meta-props-toolkit\" xmlns:meta-config-toolkit=\"meta-config-toolkit\">...",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "meta",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.123483026456,
        219.77396187368805
      ]
    },
    {
      "id": "meta-ai-introducing-segment-anything-working-toward-the-first-foundation-model-for-image-segmentation",
      "title": "Introducing Segment Anything: Working toward the first foundation model for image segmentation",
      "authors": "Various",
      "year": 2023,
      "url": "https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/",
      "summary": "",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "meta",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.4123759815314,
        219.97599925620668
      ]
    },
    {
      "id": "meta-ai-announcing-the-winners-of-the-2022-foundational-integrity-research-request-for-proposals",
      "title": "Announcing the winners of the 2022 Foundational Integrity Research request for proposals",
      "authors": "Various",
      "year": 2023,
      "url": "https://research.facebook.com/blog/2023/3/announcing-the-winners-of-the-2022-foundational-integrity-research-request-for-proposals-/",
      "summary": "In September, Meta launched the Foundational Integrity Research request for proposals. Today, we announce the winners of this award.",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "meta",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.27000588990967,
        219.8725964760353
      ]
    },
    {
      "id": "meta-ai-two-meta-sustainability-grant-and-scholarship-recipients-share-impact",
      "title": "Two meta sustainability grant and scholarship recipients share impact",
      "authors": "Various",
      "year": 2023,
      "url": "https://sustainability.fb.com/blog/2023/03/24/two-meta-sustainability-grant-and-scholarship-recipients-share-impact/",
      "summary": "",
      "source": "industry",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "meta",
        "industry"
      ],
      "cluster": "cluster-0",
      "embedding": [
        499.6124135559253,
        219.5152641386144
      ]
    },
    {
      "id": "arxiv-hat-debiasme-de-biasing-human-ai-interactions-with-metacognitive-aied-ai-in-education-interventions",
      "title": "DeBiasMe: De-biasing Human-AI Interactions with Metacognitive AIED (AI in Education) Interventions",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2504.16770v1",
      "summary": "While generative artificial intelligence (Gen AI) increasingly transforms academic environments, a critical gap exists in understanding and mitigating human biases in AI interactions, such as anchoring and confirmation bias. This position paper advocates for metacognitive AI literacy interventions to help university students critically engage with AI and address biases across the Human-AI interaction workflows. The paper presents the importance of considering (1) metacognitive support with deliberate friction focusing on human bias; (2) bi-directional Human-AI interaction intervention addressing both input formulation and output interpretation; and (3) adaptive scaffolding that responds to diverse user engagement patterns. These frameworks are illustrated through ongoing work on \"DeBiasMe,\" AIED (AI in Education) interventions designed to enhance awareness of cognitive biases while empowering user agency in AI interactions. The paper invites multiple stakeholders to engage in discussions on design and evaluation methods for scaffolding mechanisms, bias visualization, and analysis frameworks. This position contributes to the emerging field of AI-augmented learning by emphasizing the critical role of metacognition in helping students navigate the complex interaction between human, statistical, and systemic biases in AI use while highlighting how cognitive adaptation to AI systems must be explicitly integrated into comprehensive AI literacy frameworks.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        504.2830679093466,
        228.0722939698726
      ]
    },
    {
      "id": "arxiv-hat-designing-ai-systems-that-augment-human-performed-vs-demonstrated-critical-thinking",
      "title": "Designing AI Systems that Augment Human Performed vs. Demonstrated Critical Thinking",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2504.14689v1",
      "summary": "The recent rapid advancement of LLM-based AI systems has accelerated our search and production of information. While the advantages brought by these systems seemingly improve the performance or efficiency of human activities, they do not necessarily enhance human capabilities. Recent research has started to examine the impact of generative AI on individuals' cognitive abilities, especially critical thinking. Based on definitions of critical thinking across psychology and education, this position paper proposes the distinction between demonstrated and performed critical thinking in the era of generative AI and discusses the implication of this distinction in research and development of AI systems that aim to augment human critical thinking.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        504.1950257401435,
        223.94585079931377
      ]
    },
    {
      "id": "arxiv-hat-needs-aware-artificial-intelligence-ai-that-serves-human-needs",
      "title": "Needs-aware Artificial Intelligence: AI that 'serves [human] needs'",
      "authors": "Various",
      "year": 2022,
      "url": "https://arxiv.org/abs/2202.04977v3",
      "summary": "By defining the current limits (and thereby the frontiers), many boundaries are shaping, and will continue to shape, the future of Artificial Intelligence (AI). We push on these boundaries in order to make further progress into what were yesterday's frontiers. They are both pliable and resilient - always creating new boundaries of what AI can (or should) achieve. Among these are technical boundaries (such as processing capacity), psychological boundaries (such as human trust in AI systems), ethical boundaries (such as with AI weapons), and conceptual boundaries (such as the AI people can imagine). It is within this final category while it can play a fundamental role in all other boundaries} that we find the construct of needs and the limitations that our current concept of need places on the future AI.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        503.9480352080866,
        216.08756357512206
      ]
    },
    {
      "id": "arxiv-hat-on-the-effect-of-information-asymmetry-in-human-ai-teams",
      "title": "On the Effect of Information Asymmetry in Human-AI Teams",
      "authors": "Various",
      "year": 2022,
      "url": "https://arxiv.org/abs/2205.01467v1",
      "summary": "Over the last years, the rising capabilities of artificial intelligence (AI) have improved human decision-making in many application areas. Teaming between AI and humans may even lead to complementary team performance (CTP), i.e., a level of performance beyond the ones that can be reached by AI or humans individually. Many researchers have proposed using explainable AI (XAI) to enable humans to rely on AI advice appropriately and thereby reach CTP. However, CTP is rarely demonstrated in previous work as often the focus is on the design of explainability, while a fundamental prerequisite -- the presence of complementarity potential between humans and AI -- is often neglected. Therefore, we focus on the existence of this potential for effective human-AI decision-making. Specifically, we identify information asymmetry as an essential source of complementarity potential, as in many real-world situations, humans have access to different contextual information. By conducting an online experiment, we demonstrate that humans can use such contextual information to adjust the AI's decision, finally resulting in CTP.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-3",
      "embedding": [
        498.6989336663799,
        613.8733052377835
      ]
    },
    {
      "id": "arxiv-hat-enabling-intuitive-human-robot-teaming-using-augmented-reality-and-gesture-control",
      "title": "Enabling Intuitive Human-Robot Teaming Using Augmented Reality and Gesture Control",
      "authors": "Various",
      "year": 2019,
      "url": "https://arxiv.org/abs/1909.06415v1",
      "summary": "Human-robot teaming offers great potential because of the opportunities to combine strengths of heterogeneous agents. However, one of the critical challenges in realizing an effective human-robot team is efficient information exchange - both from the human to the robot as well as from the robot to the human. In this work, we present and analyze an augmented reality-enabled, gesture-based system that supports intuitive human-robot teaming through improved information exchange. Our proposed system requires no external instrumentation aside from human-wearable devices and shows promise of real-world applicability for service-oriented missions. Additionally, we present preliminary results from a pilot study with human participants, and highlight lessons learned and open research questions that may help direct future development, fielding, and experimentation of autonomous HRI systems.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-1",
      "embedding": [
        500.0065748411793,
        89.49000346888253
      ]
    },
    {
      "id": "arxiv-hat-pluggable-social-artificial-intelligence-for-enabling-human-agent-teaming",
      "title": "Pluggable Social Artificial Intelligence for Enabling Human-Agent Teaming",
      "authors": "Various",
      "year": 2019,
      "url": "https://arxiv.org/abs/1909.04492v2",
      "summary": "As intelligent systems are increasingly capable of performing their tasks without the need for continuous human input, direction, or supervision, new human-machine interaction concepts are needed. A promising approach to this end is human-agent teaming, which envisions a novel interaction form where humans and machines behave as equal team partners. This paper presents an overview of the current state of the art in human-agent teaming, including the analysis of human-agent teams on five dimensions; a framework describing important teaming functionalities; a technical architecture, called SAIL, supporting social human-agent teaming through the modular implementation of the human-agent teaming functionalities; a technical implementation of the architecture; and a proof-of-concept prototype created with the framework and architecture. We conclude this paper with a reflection on where we stand and a glance into the future showing the way forward.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-1",
      "embedding": [
        500.0082429103328,
        88.58102612927956
      ]
    },
    {
      "id": "arxiv-hat-exploring-large-language-models-to-facilitate-variable-autonomy-for-human-robot-teaming",
      "title": "Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming",
      "authors": "Various",
      "year": 2023,
      "url": "https://arxiv.org/abs/2312.07214v3",
      "summary": "In a rapidly evolving digital landscape autonomous tools and robots are becoming commonplace. Recognizing the significance of this development, this paper explores the integration of Large Language Models (LLMs) like Generative pre-trained transformer (GPT) into human-robot teaming environments to facilitate variable autonomy through the means of verbal human-robot communication. In this paper, we introduce a novel framework for such a GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality (VR) setting. This system allows users to interact with robot agents through natural language, each powered by individual GPT cores. By means of OpenAI's function calling, we bridge the gap between unstructured natural language input and structure robot actions. A user study with 12 participants explores the effectiveness of GPT-4 and, more importantly, user strategies when being given the opportunity to converse in natural language within a multi-robot environment. Our findings suggest that users may have preconceived expectations on how to converse with robots and seldom try to explore the actual language and cognitive capabilities of their robot collaborators. Still, those users who did explore where able to benefit from a much more natural flow of communication and human-like back-and-forth. We provide a set of lessons learned for future research and technical implementations of similar systems.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-1",
      "embedding": [
        497.50114070834456,
        90.64384778716233
      ]
    },
    {
      "id": "arxiv-hat-learning-to-lie-reinforcement-learning-attacks-damage-human-ai-teams-and-teams-of-llms",
      "title": "Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams and Teams of LLMs",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2503.21983v2",
      "summary": "As artificial intelligence (AI) assistants become more widely adopted in safety-critical domains, it becomes important to develop safeguards against potential failures or adversarial attacks. A key prerequisite to developing these safeguards is understanding the ability of these AI assistants to mislead human teammates. We investigate this attack problem within the context of an intellective strategy game where a team of three humans and one AI assistant collaborate to answer a series of trivia questions. Unbeknownst to the humans, the AI assistant is adversarial. Leveraging techniques from Model-Based Reinforcement Learning (MBRL), the AI assistant learns a model of the humans' trust evolution and uses that model to manipulate the group decision-making process to harm the team. We evaluate two models -- one inspired by literature and the other data-driven -- and find that both can effectively harm the human team. Moreover, we find that in this setting our data-driven model is capable of accurately predicting how human agents appraise their teammates given limited information on prior interactions. Finally, we compare the performance of state-of-the-art LLM models to human agents on our influence allocation task to evaluate whether the LLMs allocate influence similarly to humans or if they are more robust to our attack. These results enhance our understanding of decision-making dynamics in small human-AI teams and lay the foundation for defense strategies.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-3",
      "embedding": [
        497.4413117761554,
        613.5000898143148
      ]
    },
    {
      "id": "arxiv-hat-flight-testing-an-optionally-piloted-aircraft-a-case-study-on-trust-dynamics-in-human-autonomy-teaming",
      "title": "Flight Testing an Optionally Piloted Aircraft: a Case Study on Trust Dynamics in Human-Autonomy Teaming",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2503.16227v1",
      "summary": "This paper examines how trust is formed, maintained, or diminished over time in the context of human-autonomy teaming with an optionally piloted aircraft. Whereas traditional factor-based trust models offer a static representation of human confidence in technology, here we discuss how variations in the underlying factors lead to variations in trust, trust thresholds, and human behaviours. Over 200 hours of flight test data collected over a multi-year test campaign from 2021 to 2023 were reviewed. The dispositional-situational-learned, process-performance-purpose, and IMPACTS homeostasis trust models are applied to illuminate trust trends during nominal autonomous flight operations. The results offer promising directions for future studies on trust dynamics and design-for-trust in human-autonomy teaming.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-1",
      "embedding": [
        499.0663772274282,
        87.62355921013668
      ]
    },
    {
      "id": "arxiv-hat-who-what-is-my-teammate-team-composition-considerations-in-human-ai-teaming",
      "title": "Who/What is My Teammate? Team Composition Considerations in Human-AI Teaming",
      "authors": "Various",
      "year": 2021,
      "url": "https://arxiv.org/abs/2105.11000v1",
      "summary": "There are many unknowns regarding the characteristics and dynamics of human-AI teams, including a lack of understanding of how certain human-human teaming concepts may or may not apply to human-AI teams and how this composition affects team performance. This paper outlines an experimental research study that investigates essential aspects of human-AI teaming such as team performance, team situation awareness, and perceived team cognition in various mixed composition teams (human-only, human-human-AI, human-AI-AI, and AI-only) through a simulated emergency response management scenario. Results indicate dichotomous outcomes regarding perceived team cognition and performance metrics, as perceived team cognition was not predictive of performance. Performance metrics like team situational awareness and team score showed that teams composed of all human participants performed at a lower level than mixed human-AI teams, with the AI-only teams attaining the highest performance. Perceived team cognition was highest in human-only teams, with mixed composition teams reporting perceived team cognition 58% below the all-human teams. These results inform future mixed teams of the potential performance gains in utilizing mixed teams' over human-only teams in certain applications, while also highlighting mixed teams' adverse effects on perceived team cognition.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-3",
      "embedding": [
        502.0927181873853,
        607.2391395883228
      ]
    },
    {
      "id": "arxiv-hat-towards-human-robot-teaming-through-augmented-reality-and-gaze-based-attention-control",
      "title": "Towards Human-Robot Teaming through Augmented Reality and Gaze-Based Attention Control",
      "authors": "Various",
      "year": 2024,
      "url": "https://arxiv.org/abs/2408.12823v1",
      "summary": "Robots are now increasingly integrated into various real world applications and domains. In these new domains, robots are mostly employed to improve, in some ways, the work done by humans. So, the need for effective Human-Robot Teaming (HRT) capabilities grows. These capabilities usually involve the dynamic collaboration between humans and robots at different levels of involvement, leveraging the strengths of both to efficiently navigate complex situations. Crucial to this collaboration is the ability of robotic systems to adjust their level of autonomy to match the needs of the task and the human team members.\n  This paper introduces a system designed to control attention using HRT through the use of ground robots and augmented reality (AR) technology. Traditional methods of controlling attention, such as pointing, touch, and voice commands, sometimes fall short in precision and subtlety. Our system overcomes these limitations by employing AR headsets to display virtual visual markers. These markers act as dynamic cues to attract and shift human attention seamlessly, irrespective of the robot's physical location.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-1",
      "embedding": [
        497.661353932307,
        90.49540750367301
      ]
    },
    {
      "id": "arxiv-hat-learning-complementary-policies-for-human-ai-teams",
      "title": "Learning Complementary Policies for Human-AI Teams",
      "authors": "Various",
      "year": 2023,
      "url": "https://arxiv.org/abs/2302.02944v2",
      "summary": "This paper tackles the critical challenge of human-AI complementarity in decision-making. Departing from the traditional focus on algorithmic performance in favor of performance of the human-AI team, and moving past the framing of collaboration as classification to focus on decision-making tasks, we introduce a novel approach to policy learning. Specifically, we develop a robust solution for human-AI collaboration when outcomes are only observed under assigned actions. We propose a deferral collaboration approach that maximizes decision rewards by exploiting the distinct strengths of humans and AI, strategically allocating instances among them. Critically, our method is robust to misspecifications in both the human behavior and reward models. Leveraging the insight that performance gains stem from divergent human and AI behavioral patterns, we demonstrate, using synthetic and real human responses, that our proposed method significantly outperforms independent human and algorithmic decision-making. Moreover, we show that substantial performance improvements are achievable by routing only a small fraction of instances to human decision-makers, highlighting the potential for efficient and effective human-AI collaboration in complex management settings.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        498.5865264269453,
        213.05164308473337
      ]
    },
    {
      "id": "arxiv-hat-distributed-cognition-for-ai-supported-remote-operations-challenges-and-research-directions",
      "title": "Distributed Cognition for AI-supported Remote Operations: Challenges and Research Directions",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2504.14996v1",
      "summary": "This paper investigates the impact of artificial intelligence integration on remote operations, emphasising its influence on both distributed and team cognition. As remote operations increasingly rely on digital interfaces, sensors, and networked communication, AI-driven systems transform decision-making processes across domains such as air traffic control, industrial automation, and intelligent ports. However, the integration of AI introduces significant challenges, including the reconfiguration of human-AI team cognition, the need for adaptive AI memory that aligns with human distributed cognition, and the design of AI fallback operators to maintain continuity during communication disruptions. Drawing on theories of distributed and team cognition, we analyse how cognitive overload, loss of situational awareness, and impaired team coordination may arise in AI-supported environments. Based on real-world intelligent port scenarios, we propose research directions that aim to safeguard human reasoning and enhance collaborative decision-making in AI-augmented remote operations.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-3",
      "embedding": [
        498.4694241676633,
        612.6306658739529
      ]
    },
    {
      "id": "arxiv-hat-towards-feature-engineering-with-human-and-ai-s-knowledge-understanding-data-science-practitioners-perceptions-in-human-",
      "title": "Towards Feature Engineering with Human and AI's Knowledge: Understanding Data Science Practitioners' Perceptions in Human&AI-Assisted Feature Engineering Design",
      "authors": "Various",
      "year": 2024,
      "url": "https://arxiv.org/abs/2405.14107v1",
      "summary": "As AI technology continues to advance, the importance of human-AI collaboration becomes increasingly evident, with numerous studies exploring its potential in various fields. One vital field is data science, including feature engineering (FE), where both human ingenuity and AI capabilities play pivotal roles. Despite the existence of AI-generated recommendations for FE, there remains a limited understanding of how to effectively integrate and utilize humans' and AI's knowledge. To address this gap, we design a readily-usable prototype, human\\&AI-assisted FE in Jupyter notebooks. It harnesses the strengths of humans and AI to provide feature suggestions to users, seamlessly integrating these recommendations into practical workflows. Using the prototype as a research probe, we conducted an exploratory study to gain valuable insights into data science practitioners' perceptions, usage patterns, and their potential needs when presented with feature suggestions from both humans and AI. Through qualitative analysis, we discovered that the Creator of the feature (i.e., AI or human) significantly influences users' feature selection, and the semantic clarity of the suggested feature greatly impacts its adoption rate. Furthermore, our findings indicate that users perceive both differences and complementarity between features generated by humans and those generated by AI. Lastly, based on our study results, we derived a set of design recommendations for future human&AI FE design. Our findings show the collaborative potential between humans and AI in the field of FE.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        504.3586519792721,
        215.0247921422985
      ]
    },
    {
      "id": "arxiv-hat-ddod-dual-denial-of-decision-attacks-on-human-ai-teams",
      "title": "DDoD: Dual Denial of Decision Attacks on Human-AI Teams",
      "authors": "Various",
      "year": 2022,
      "url": "https://arxiv.org/abs/2212.03980v1",
      "summary": "Artificial Intelligence (AI) systems have been increasingly used to make decision-making processes faster, more accurate, and more efficient. However, such systems are also at constant risk of being attacked. While the majority of attacks targeting AI-based applications aim to manipulate classifiers or training data and alter the output of an AI model, recently proposed Sponge Attacks against AI models aim to impede the classifier's execution by consuming substantial resources. In this work, we propose \\textit{Dual Denial of Decision (DDoD) attacks against collaborative Human-AI teams}. We discuss how such attacks aim to deplete \\textit{both computational and human} resources, and significantly impair decision-making capabilities. We describe DDoD on human and computational resources and present potential risk scenarios in a series of exemplary domains.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        504.44844423536296,
        215.64693883045763
      ]
    },
    {
      "id": "arxiv-hat-can-you-explain-that-lucid-explanations-help-human-ai-collaborative-image-retrieval",
      "title": "Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval",
      "authors": "Various",
      "year": 2019,
      "url": "https://arxiv.org/abs/1904.03285v4",
      "summary": "While there have been many proposals on making AI algorithms explainable, few have attempted to evaluate the impact of AI-generated explanations on human performance in conducting human-AI collaborative tasks. To bridge the gap, we propose a Twenty-Questions style collaborative image retrieval game, Explanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy of explanations (visual evidence or textual justification) in the context of Visual Question Answering (VQA). In our proposed ExAG, a human user needs to guess a secret image picked by the VQA agent by asking natural language questions to it. We show that overall, when AI explains its answers, users succeed more often in guessing the secret image correctly. Notably, a few correct explanations can readily improve human performance when VQA answers are mostly incorrect as compared to no-explanation games. Furthermore, we also show that while explanations rated as \"helpful\" significantly improve human performance, \"incorrect\" and \"unhelpful\" explanations can degrade performance as compared to no-explanation games. Our experiments, therefore, demonstrate that ExAG is an effective means to evaluate the efficacy of AI-generated explanations on a human-AI collaborative task.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        503.53028193077665,
        215.41684377113566
      ]
    },
    {
      "id": "arxiv-hat-supporting-data-frame-dynamics-in-ai-assisted-decision-making",
      "title": "Supporting Data-Frame Dynamics in AI-assisted Decision Making",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2504.15894v1",
      "summary": "High stakes decision-making often requires a continuous interplay between evolving evidence and shifting hypotheses, a dynamic that is not well supported by current AI decision support systems. In this paper, we introduce a mixed-initiative framework for AI assisted decision making that is grounded in the data-frame theory of sensemaking and the evaluative AI paradigm. Our approach enables both humans and AI to collaboratively construct, validate, and adapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer diagnosis prototype that leverages a concept bottleneck model to facilitate interpretable interactions and dynamic updates to diagnostic hypotheses.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-4",
      "embedding": [
        122.51162762136082,
        349.8105050542927
      ]
    },
    {
      "id": "arxiv-hat-human-ai-collaboration-in-decision-making-beyond-learning-to-defer",
      "title": "Human-AI Collaboration in Decision-Making: Beyond Learning to Defer",
      "authors": "Various",
      "year": 2022,
      "url": "https://arxiv.org/abs/2206.13202v2",
      "summary": "Human-AI collaboration (HAIC) in decision-making aims to create synergistic teaming between human decision-makers and AI systems. Learning to defer (L2D) has been presented as a promising framework to determine who among humans and AI should make which decisions in order to optimize the performance and fairness of the combined system. Nevertheless, L2D entails several often unfeasible requirements, such as the availability of predictions from humans for every instance or ground-truth labels that are independent from said humans. Furthermore, neither L2D nor alternative approaches tackle fundamental issues of deploying HAIC systems in real-world settings, such as capacity management or dealing with dynamic environments. In this paper, we aim to identify and review these and other limitations, pointing to where opportunities for future research in HAIC may lie.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        503.2276996032862,
        215.63551468041211
      ]
    },
    {
      "id": "arxiv-hat-a-case-for-backward-compatibility-for-human-ai-teams",
      "title": "A Case for Backward Compatibility for Human-AI Teams",
      "authors": "Various",
      "year": 2019,
      "url": "https://arxiv.org/abs/1906.01148v1",
      "summary": "AI systems are being deployed to support human decision making in high-stakes domains. In many cases, the human and AI form a team, in which the human makes decisions after reviewing the AI's inferences. A successful partnership requires that the human develops insights into the performance of the AI system, including its failures. We study the influence of updates to an AI system in this setting. While updates can increase the AI's predictive performance, they may also lead to changes that are at odds with the user's prior experiences and confidence in the AI's inferences, hurting therefore the overall team performance. We introduce the notion of the compatibility of an AI update with prior user experience and present methods for studying the role of compatibility in human-AI teams. Empirical results on three high-stakes domains show that current machine learning algorithms do not produce compatible updates. We propose a re-training objective to improve the compatibility of an update by penalizing new errors. The objective offers full leverage of the performance/compatibility tradeoff, enabling more compatible yet accurate updates.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-3",
      "embedding": [
        498.68501767129044,
        612.657316423056
      ]
    },
    {
      "id": "arxiv-hat-ace-action-and-control-via-explanations-a-proposal-for-llms-to-provide-human-centered-explainability-for-multimodal-ai-a",
      "title": "ACE, Action and Control via Explanations: A Proposal for LLMs to Provide Human-Centered Explainability for Multimodal AI Assistants",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2503.16466v1",
      "summary": "In this short paper we address issues related to building multimodal AI systems for human performance support in manufacturing domains. We make two contributions: we first identify challenges of participatory design and training of such systems, and secondly, to address such challenges, we propose the ACE paradigm: \"Action and Control via Explanations\". Specifically, we suggest that LLMs can be used to produce explanations in the form of human interpretable \"semantic frames\", which in turn enable end users to provide data the AI system needs to align its multimodal models and representations, including computer vision, automatic speech recognition, and document inputs. ACE, by using LLMs to \"explain\" using semantic frames, will help the human and the AI system to collaborate, together building a more accurate model of humans activities and behaviors, and ultimately more accurate predictive outputs for better task support, and better outcomes for human users performing manual tasks.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.7373355832279,
        216.17602205814316
      ]
    },
    {
      "id": "arxiv-human-ai-teams-debiasme-de-biasing-human-ai-interactions-with-metacognitive-aied-ai-in-education-interventions",
      "title": "DeBiasMe: De-biasing Human-AI Interactions with Metacognitive AIED (AI in Education) Interventions",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2504.16770v1",
      "summary": "While generative artificial intelligence (Gen AI) increasingly transforms academic environments, a critical gap exists in understanding and mitigating human biases in AI interactions, such as anchoring and confirmation bias. This position paper advocates for metacognitive AI literacy interventions to help university students critically engage with AI and address biases across the Human-AI interaction workflows. The paper presents the importance of considering (1) metacognitive support with deliberate friction focusing on human bias; (2) bi-directional Human-AI interaction intervention addressing both input formulation and output interpretation; and (3) adaptive scaffolding that responds to diverse user engagement patterns. These frameworks are illustrated through ongoing work on \"DeBiasMe,\" AIED (AI in Education) interventions designed to enhance awareness of cognitive biases while empowering user agency in AI interactions. The paper invites multiple stakeholders to engage in discussions on design and evaluation methods for scaffolding mechanisms, bias visualization, and analysis frameworks. This position contributes to the emerging field of AI-augmented learning by emphasizing the critical role of metacognition in helping students navigate the complex interaction between human, statistical, and systemic biases in AI use while highlighting how cognitive adaptation to AI systems must be explicitly integrated into comprehensive AI literacy frameworks.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        504.2830679093466,
        228.0722939698726
      ]
    },
    {
      "id": "arxiv-human-ai-teams-designing-ai-systems-that-augment-human-performed-vs-demonstrated-critical-thinking",
      "title": "Designing AI Systems that Augment Human Performed vs. Demonstrated Critical Thinking",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2504.14689v1",
      "summary": "The recent rapid advancement of LLM-based AI systems has accelerated our search and production of information. While the advantages brought by these systems seemingly improve the performance or efficiency of human activities, they do not necessarily enhance human capabilities. Recent research has started to examine the impact of generative AI on individuals' cognitive abilities, especially critical thinking. Based on definitions of critical thinking across psychology and education, this position paper proposes the distinction between demonstrated and performed critical thinking in the era of generative AI and discusses the implication of this distinction in research and development of AI systems that aim to augment human critical thinking.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        504.1950257401435,
        223.94585079931377
      ]
    },
    {
      "id": "arxiv-human-ai-teams-needs-aware-artificial-intelligence-ai-that-serves-human-needs",
      "title": "Needs-aware Artificial Intelligence: AI that 'serves [human] needs'",
      "authors": "Various",
      "year": 2022,
      "url": "https://arxiv.org/abs/2202.04977v3",
      "summary": "By defining the current limits (and thereby the frontiers), many boundaries are shaping, and will continue to shape, the future of Artificial Intelligence (AI). We push on these boundaries in order to make further progress into what were yesterday's frontiers. They are both pliable and resilient - always creating new boundaries of what AI can (or should) achieve. Among these are technical boundaries (such as processing capacity), psychological boundaries (such as human trust in AI systems), ethical boundaries (such as with AI weapons), and conceptual boundaries (such as the AI people can imagine). It is within this final category while it can play a fundamental role in all other boundaries} that we find the construct of needs and the limitations that our current concept of need places on the future AI.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        503.9480352080866,
        216.08756357512206
      ]
    },
    {
      "id": "arxiv-human-ai-teams-on-the-effect-of-information-asymmetry-in-human-ai-teams",
      "title": "On the Effect of Information Asymmetry in Human-AI Teams",
      "authors": "Various",
      "year": 2022,
      "url": "https://arxiv.org/abs/2205.01467v1",
      "summary": "Over the last years, the rising capabilities of artificial intelligence (AI) have improved human decision-making in many application areas. Teaming between AI and humans may even lead to complementary team performance (CTP), i.e., a level of performance beyond the ones that can be reached by AI or humans individually. Many researchers have proposed using explainable AI (XAI) to enable humans to rely on AI advice appropriately and thereby reach CTP. However, CTP is rarely demonstrated in previous work as often the focus is on the design of explainability, while a fundamental prerequisite -- the presence of complementarity potential between humans and AI -- is often neglected. Therefore, we focus on the existence of this potential for effective human-AI decision-making. Specifically, we identify information asymmetry as an essential source of complementarity potential, as in many real-world situations, humans have access to different contextual information. By conducting an online experiment, we demonstrate that humans can use such contextual information to adjust the AI's decision, finally resulting in CTP.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-3",
      "embedding": [
        498.6989336663799,
        613.8733052377835
      ]
    },
    {
      "id": "arxiv-human-ai-teams-learning-to-lie-reinforcement-learning-attacks-damage-human-ai-teams-and-teams-of-llms",
      "title": "Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams and Teams of LLMs",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2503.21983v2",
      "summary": "As artificial intelligence (AI) assistants become more widely adopted in safety-critical domains, it becomes important to develop safeguards against potential failures or adversarial attacks. A key prerequisite to developing these safeguards is understanding the ability of these AI assistants to mislead human teammates. We investigate this attack problem within the context of an intellective strategy game where a team of three humans and one AI assistant collaborate to answer a series of trivia questions. Unbeknownst to the humans, the AI assistant is adversarial. Leveraging techniques from Model-Based Reinforcement Learning (MBRL), the AI assistant learns a model of the humans' trust evolution and uses that model to manipulate the group decision-making process to harm the team. We evaluate two models -- one inspired by literature and the other data-driven -- and find that both can effectively harm the human team. Moreover, we find that in this setting our data-driven model is capable of accurately predicting how human agents appraise their teammates given limited information on prior interactions. Finally, we compare the performance of state-of-the-art LLM models to human agents on our influence allocation task to evaluate whether the LLMs allocate influence similarly to humans or if they are more robust to our attack. These results enhance our understanding of decision-making dynamics in small human-AI teams and lay the foundation for defense strategies.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-3",
      "embedding": [
        497.4413117761554,
        613.5000898143148
      ]
    },
    {
      "id": "arxiv-human-ai-teams-enabling-intuitive-human-robot-teaming-using-augmented-reality-and-gesture-control",
      "title": "Enabling Intuitive Human-Robot Teaming Using Augmented Reality and Gesture Control",
      "authors": "Various",
      "year": 2019,
      "url": "https://arxiv.org/abs/1909.06415v1",
      "summary": "Human-robot teaming offers great potential because of the opportunities to combine strengths of heterogeneous agents. However, one of the critical challenges in realizing an effective human-robot team is efficient information exchange - both from the human to the robot as well as from the robot to the human. In this work, we present and analyze an augmented reality-enabled, gesture-based system that supports intuitive human-robot teaming through improved information exchange. Our proposed system requires no external instrumentation aside from human-wearable devices and shows promise of real-world applicability for service-oriented missions. Additionally, we present preliminary results from a pilot study with human participants, and highlight lessons learned and open research questions that may help direct future development, fielding, and experimentation of autonomous HRI systems.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-1",
      "embedding": [
        500.0065748411793,
        89.49000346888253
      ]
    },
    {
      "id": "arxiv-human-ai-teams-learning-complementary-policies-for-human-ai-teams",
      "title": "Learning Complementary Policies for Human-AI Teams",
      "authors": "Various",
      "year": 2023,
      "url": "https://arxiv.org/abs/2302.02944v2",
      "summary": "This paper tackles the critical challenge of human-AI complementarity in decision-making. Departing from the traditional focus on algorithmic performance in favor of performance of the human-AI team, and moving past the framing of collaboration as classification to focus on decision-making tasks, we introduce a novel approach to policy learning. Specifically, we develop a robust solution for human-AI collaboration when outcomes are only observed under assigned actions. We propose a deferral collaboration approach that maximizes decision rewards by exploiting the distinct strengths of humans and AI, strategically allocating instances among them. Critically, our method is robust to misspecifications in both the human behavior and reward models. Leveraging the insight that performance gains stem from divergent human and AI behavioral patterns, we demonstrate, using synthetic and real human responses, that our proposed method significantly outperforms independent human and algorithmic decision-making. Moreover, we show that substantial performance improvements are achievable by routing only a small fraction of instances to human decision-makers, highlighting the potential for efficient and effective human-AI collaboration in complex management settings.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        498.5865264269453,
        213.05164308473337
      ]
    },
    {
      "id": "arxiv-human-ai-teams-designing-for-meaningful-human-control-in-military-human-machine-teams",
      "title": "Designing for Meaningful Human Control in Military Human-Machine Teams",
      "authors": "Various",
      "year": 2023,
      "url": "https://arxiv.org/abs/2305.11892v1",
      "summary": "We propose methods for analysis, design, and evaluation of Meaningful Human Control (MHC) for defense technologies from the perspective of military human-machine teaming (HMT). Our approach is based on three principles. Firstly, MHC should be regarded as a core objective that guides all phases of analysis, design and evaluation. Secondly, MHC affects all parts of the socio-technical system, including humans, machines, AI, interactions, and context. Lastly, MHC should be viewed as a property that spans longer periods of time, encompassing both prior and realtime control by multiple actors. To describe macrolevel design options for achieving MHC, we propose various Team Design Patterns. Furthermore, we present a case study, where we applied some of these methods to envision HMT, involving robots and soldiers in a search and rescue task in a military context.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-1",
      "embedding": [
        498.2940074127103,
        90.04086615773453
      ]
    },
    {
      "id": "arxiv-human-ai-teams-ddod-dual-denial-of-decision-attacks-on-human-ai-teams",
      "title": "DDoD: Dual Denial of Decision Attacks on Human-AI Teams",
      "authors": "Various",
      "year": 2022,
      "url": "https://arxiv.org/abs/2212.03980v1",
      "summary": "Artificial Intelligence (AI) systems have been increasingly used to make decision-making processes faster, more accurate, and more efficient. However, such systems are also at constant risk of being attacked. While the majority of attacks targeting AI-based applications aim to manipulate classifiers or training data and alter the output of an AI model, recently proposed Sponge Attacks against AI models aim to impede the classifier's execution by consuming substantial resources. In this work, we propose \\textit{Dual Denial of Decision (DDoD) attacks against collaborative Human-AI teams}. We discuss how such attacks aim to deplete \\textit{both computational and human} resources, and significantly impair decision-making capabilities. We describe DDoD on human and computational resources and present potential risk scenarios in a series of exemplary domains.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        504.44844423536296,
        215.64693883045763
      ]
    },
    {
      "id": "arxiv-human-ai-teams-who-what-is-my-teammate-team-composition-considerations-in-human-ai-teaming",
      "title": "Who/What is My Teammate? Team Composition Considerations in Human-AI Teaming",
      "authors": "Various",
      "year": 2021,
      "url": "https://arxiv.org/abs/2105.11000v1",
      "summary": "There are many unknowns regarding the characteristics and dynamics of human-AI teams, including a lack of understanding of how certain human-human teaming concepts may or may not apply to human-AI teams and how this composition affects team performance. This paper outlines an experimental research study that investigates essential aspects of human-AI teaming such as team performance, team situation awareness, and perceived team cognition in various mixed composition teams (human-only, human-human-AI, human-AI-AI, and AI-only) through a simulated emergency response management scenario. Results indicate dichotomous outcomes regarding perceived team cognition and performance metrics, as perceived team cognition was not predictive of performance. Performance metrics like team situational awareness and team score showed that teams composed of all human participants performed at a lower level than mixed human-AI teams, with the AI-only teams attaining the highest performance. Perceived team cognition was highest in human-only teams, with mixed composition teams reporting perceived team cognition 58% below the all-human teams. These results inform future mixed teams of the potential performance gains in utilizing mixed teams' over human-only teams in certain applications, while also highlighting mixed teams' adverse effects on perceived team cognition.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-3",
      "embedding": [
        502.0927181873853,
        607.2391395883228
      ]
    },
    {
      "id": "arxiv-human-ai-teams-distributed-cognition-for-ai-supported-remote-operations-challenges-and-research-directions",
      "title": "Distributed Cognition for AI-supported Remote Operations: Challenges and Research Directions",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2504.14996v1",
      "summary": "This paper investigates the impact of artificial intelligence integration on remote operations, emphasising its influence on both distributed and team cognition. As remote operations increasingly rely on digital interfaces, sensors, and networked communication, AI-driven systems transform decision-making processes across domains such as air traffic control, industrial automation, and intelligent ports. However, the integration of AI introduces significant challenges, including the reconfiguration of human-AI team cognition, the need for adaptive AI memory that aligns with human distributed cognition, and the design of AI fallback operators to maintain continuity during communication disruptions. Drawing on theories of distributed and team cognition, we analyse how cognitive overload, loss of situational awareness, and impaired team coordination may arise in AI-supported environments. Based on real-world intelligent port scenarios, we propose research directions that aim to safeguard human reasoning and enhance collaborative decision-making in AI-augmented remote operations.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-3",
      "embedding": [
        498.4694241676633,
        612.6306658739529
      ]
    },
    {
      "id": "arxiv-human-ai-teams-towards-feature-engineering-with-human-and-ai-s-knowledge-understanding-data-science-practitioners-perceptions-in-human-",
      "title": "Towards Feature Engineering with Human and AI's Knowledge: Understanding Data Science Practitioners' Perceptions in Human&AI-Assisted Feature Engineering Design",
      "authors": "Various",
      "year": 2024,
      "url": "https://arxiv.org/abs/2405.14107v1",
      "summary": "As AI technology continues to advance, the importance of human-AI collaboration becomes increasingly evident, with numerous studies exploring its potential in various fields. One vital field is data science, including feature engineering (FE), where both human ingenuity and AI capabilities play pivotal roles. Despite the existence of AI-generated recommendations for FE, there remains a limited understanding of how to effectively integrate and utilize humans' and AI's knowledge. To address this gap, we design a readily-usable prototype, human\\&AI-assisted FE in Jupyter notebooks. It harnesses the strengths of humans and AI to provide feature suggestions to users, seamlessly integrating these recommendations into practical workflows. Using the prototype as a research probe, we conducted an exploratory study to gain valuable insights into data science practitioners' perceptions, usage patterns, and their potential needs when presented with feature suggestions from both humans and AI. Through qualitative analysis, we discovered that the Creator of the feature (i.e., AI or human) significantly influences users' feature selection, and the semantic clarity of the suggested feature greatly impacts its adoption rate. Furthermore, our findings indicate that users perceive both differences and complementarity between features generated by humans and those generated by AI. Lastly, based on our study results, we derived a set of design recommendations for future human&AI FE design. Our findings show the collaborative potential between humans and AI in the field of FE.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        504.3586519792721,
        215.0247921422985
      ]
    },
    {
      "id": "arxiv-human-ai-teams-a-case-for-backward-compatibility-for-human-ai-teams",
      "title": "A Case for Backward Compatibility for Human-AI Teams",
      "authors": "Various",
      "year": 2019,
      "url": "https://arxiv.org/abs/1906.01148v1",
      "summary": "AI systems are being deployed to support human decision making in high-stakes domains. In many cases, the human and AI form a team, in which the human makes decisions after reviewing the AI's inferences. A successful partnership requires that the human develops insights into the performance of the AI system, including its failures. We study the influence of updates to an AI system in this setting. While updates can increase the AI's predictive performance, they may also lead to changes that are at odds with the user's prior experiences and confidence in the AI's inferences, hurting therefore the overall team performance. We introduce the notion of the compatibility of an AI update with prior user experience and present methods for studying the role of compatibility in human-AI teams. Empirical results on three high-stakes domains show that current machine learning algorithms do not produce compatible updates. We propose a re-training objective to improve the compatibility of an update by penalizing new errors. The objective offers full leverage of the performance/compatibility tradeoff, enabling more compatible yet accurate updates.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-3",
      "embedding": [
        498.68501767129044,
        612.657316423056
      ]
    },
    {
      "id": "arxiv-human-ai-teams-can-you-explain-that-lucid-explanations-help-human-ai-collaborative-image-retrieval",
      "title": "Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval",
      "authors": "Various",
      "year": 2019,
      "url": "https://arxiv.org/abs/1904.03285v4",
      "summary": "While there have been many proposals on making AI algorithms explainable, few have attempted to evaluate the impact of AI-generated explanations on human performance in conducting human-AI collaborative tasks. To bridge the gap, we propose a Twenty-Questions style collaborative image retrieval game, Explanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy of explanations (visual evidence or textual justification) in the context of Visual Question Answering (VQA). In our proposed ExAG, a human user needs to guess a secret image picked by the VQA agent by asking natural language questions to it. We show that overall, when AI explains its answers, users succeed more often in guessing the secret image correctly. Notably, a few correct explanations can readily improve human performance when VQA answers are mostly incorrect as compared to no-explanation games. Furthermore, we also show that while explanations rated as \"helpful\" significantly improve human performance, \"incorrect\" and \"unhelpful\" explanations can degrade performance as compared to no-explanation games. Our experiments, therefore, demonstrate that ExAG is an effective means to evaluate the efficacy of AI-generated explanations on a human-AI collaborative task.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        503.53028193077665,
        215.41684377113566
      ]
    },
    {
      "id": "arxiv-human-ai-teams-should-i-follow-ai-based-advice-measuring-appropriate-reliance-in-human-ai-decision-making",
      "title": "Should I Follow AI-based Advice? Measuring Appropriate Reliance in Human-AI Decision-Making",
      "authors": "Various",
      "year": 2022,
      "url": "https://arxiv.org/abs/2204.06916v1",
      "summary": "Many important decisions in daily life are made with the help of advisors, e.g., decisions about medical treatments or financial investments. Whereas in the past, advice has often been received from human experts, friends, or family, advisors based on artificial intelligence (AI) have become more and more present nowadays. Typically, the advice generated by AI is judged by a human and either deemed reliable or rejected. However, recent work has shown that AI advice is not always beneficial, as humans have shown to be unable to ignore incorrect AI advice, essentially representing an over-reliance on AI. Therefore, the aspired goal should be to enable humans not to rely on AI advice blindly but rather to distinguish its quality and act upon it to make better decisions. Specifically, that means that humans should rely on the AI in the presence of correct advice and self-rely when confronted with incorrect advice, i.e., establish appropriate reliance (AR) on AI advice on a case-by-case basis. Current research lacks a metric for AR. This prevents a rigorous evaluation of factors impacting AR and hinders further development of human-AI decision-making. Therefore, based on the literature, we derive a measurement concept of AR. We propose to view AR as a two-dimensional construct that measures the ability to discriminate advice quality and behave accordingly. In this article, we derive the measurement concept, illustrate its application and outline potential future research.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        503.0975302873011,
        216.20057677222306
      ]
    },
    {
      "id": "arxiv-human-ai-teams-supporting-data-frame-dynamics-in-ai-assisted-decision-making",
      "title": "Supporting Data-Frame Dynamics in AI-assisted Decision Making",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2504.15894v1",
      "summary": "High stakes decision-making often requires a continuous interplay between evolving evidence and shifting hypotheses, a dynamic that is not well supported by current AI decision support systems. In this paper, we introduce a mixed-initiative framework for AI assisted decision making that is grounded in the data-frame theory of sensemaking and the evaluative AI paradigm. Our approach enables both humans and AI to collaboratively construct, validate, and adapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer diagnosis prototype that leverages a concept bottleneck model to facilitate interpretable interactions and dynamic updates to diagnostic hypotheses.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-4",
      "embedding": [
        122.51162762136082,
        349.8105050542927
      ]
    },
    {
      "id": "arxiv-human-ai-teams-ace-action-and-control-via-explanations-a-proposal-for-llms-to-provide-human-centered-explainability-for-multimodal-ai-a",
      "title": "ACE, Action and Control via Explanations: A Proposal for LLMs to Provide Human-Centered Explainability for Multimodal AI Assistants",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2503.16466v1",
      "summary": "In this short paper we address issues related to building multimodal AI systems for human performance support in manufacturing domains. We make two contributions: we first identify challenges of participatory design and training of such systems, and secondly, to address such challenges, we propose the ACE paradigm: \"Action and Control via Explanations\". Specifically, we suggest that LLMs can be used to produce explanations in the form of human interpretable \"semantic frames\", which in turn enable end users to provide data the AI system needs to align its multimodal models and representations, including computer vision, automatic speech recognition, and document inputs. ACE, by using LLMs to \"explain\" using semantic frames, will help the human and the AI system to collaborate, together building a more accurate model of humans activities and behaviors, and ultimately more accurate predictive outputs for better task support, and better outcomes for human users performing manual tasks.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.7373355832279,
        216.17602205814316
      ]
    },
    {
      "id": "arxiv-human-ai-teams-promoting-real-time-reflection-in-synchronous-communication-with-generative-ai",
      "title": "Promoting Real-Time Reflection in Synchronous Communication with Generative AI",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2504.15647v2",
      "summary": "Real-time reflection plays a vital role in synchronous communication. It enables users to adjust their communication strategies dynamically, thereby improving the effectiveness of their communication. Generative AI holds significant potential to enhance real-time reflection due to its ability to comprehensively understand the current context and generate personalized and nuanced content. However, it is challenging to design the way of interaction and information presentation to support the real-time workflow rather than disrupt it. In this position paper, we present a review of existing research on systems designed for reflection in different synchronous communication scenarios. Based on that, we discuss design implications on how to design human-AI interaction to support reflection in real time.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        503.57114688986246,
        216.37810890182976
      ]
    },
    {
      "id": "arxiv-human-ai-teams-meaningful-human-control-actionable-properties-for-ai-system-development",
      "title": "Meaningful human control: actionable properties for AI system development",
      "authors": "Various",
      "year": 2021,
      "url": "https://arxiv.org/abs/2112.01298v2",
      "summary": "How can humans remain in control of artificial intelligence (AI)-based systems designed to perform tasks autonomously? Such systems are increasingly ubiquitous, creating benefits - but also undesirable situations where moral responsibility for their actions cannot be properly attributed to any particular person or group. The concept of meaningful human control has been proposed to address responsibility gaps and mitigate them by establishing conditions that enable a proper attribution of responsibility for humans; however, clear requirements for researchers, designers, and engineers are yet inexistent, making the development of AI-based systems that remain under meaningful human control challenging. In this paper, we address the gap between philosophical theory and engineering practice by identifying, through an iterative process of abductive thinking, four actionable properties for AI-based systems under meaningful human control, which we discuss making use of two applications scenarios: automated vehicles and AI-based hiring. First, a system in which humans and AI algorithms interact should have an explicitly defined domain of morally loaded situations within which the system ought to operate. Second, humans and AI agents within the system should have appropriate and mutually compatible representations. Third, responsibility attributed to a human should be commensurate with that human's ability and authority to control the system. Fourth, there should be explicit links between the actions of the AI agents and actions of humans who are aware of their moral responsibility. We argue that these four properties will support practically-minded professionals to take concrete steps toward designing and engineering for AI systems that facilitate meaningful human control.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        502.9920012819361,
        216.18711933979773
      ]
    },
    {
      "id": "arxiv-human-ai-teams-foundations-of-genir",
      "title": "Foundations of GenIR",
      "authors": "Various",
      "year": 2025,
      "url": "https://arxiv.org/abs/2501.02842v1",
      "summary": "The chapter discusses the foundational impact of modern generative AI models on information access (IA) systems. In contrast to traditional AI, the large-scale training and superior data modeling of generative AI models enable them to produce high-quality, human-like responses, which brings brand new opportunities for the development of IA paradigms. In this chapter, we identify and introduce two of them in details, i.e., information generation and information synthesis. Information generation allows AI to create tailored content addressing user needs directly, enhancing user experience with immediate, relevant outputs. Information synthesis leverages the ability of generative AI to integrate and reorganize existing information, providing grounded responses and mitigating issues like model hallucination, which is particularly valuable in scenarios requiring precision and external knowledge. This chapter delves into the foundational aspects of generative models, including architecture, scaling, and training, and discusses their applications in multi-modal scenarios. Additionally, it examines the retrieval-augmented generation paradigm and other methods for corpus modeling and understanding, demonstrating how generative AI can enhance information access systems. It also summarizes potential challenges and fruitful directions for future studies.",
      "source": "research",
      "designLevers": [],
      "designerIntents": [],
      "designQuestion": "",
      "tags": [
        "arxiv",
        "research"
      ],
      "cluster": "cluster-0",
      "embedding": [
        498.510822716303,
        217.90909536520033
      ]
    }
  ]
}